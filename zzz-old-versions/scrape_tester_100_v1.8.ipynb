{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import streamlit as st\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamped_filename(base_filename): # base_filename (str): The base name of the file without the extension.\n",
    "    \"\"\"Generate a filename with a timestamp to uniquely identify the file.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\" # str: The timestamped filename including the original base name and extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleSearchLoader: # Attributes: query (str): The search query string.\n",
    "    \"\"\"A class to load and process Google search results.\"\"\"\n",
    "    def __init__(self, query): # query (str): The search query string.\n",
    "        \"\"\"Initialize the GoogleSearchLoader object with a search query.\"\"\"\n",
    "        self.query = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def load_and_scroll(self):\n",
    "        \"\"\"Load search results and scroll until at least 100 items or no new items are found\n",
    "        and return the HTML content of the loaded pages.\"\"\"\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.get(f\"https://www.google.com/search?q={quote_plus(self.query)}\")\n",
    "\n",
    "        num_results = 0\n",
    "        collected_html = \"\"\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        \n",
    "        while num_results < 100:\n",
    "            # Wait for the search results to load\n",
    "            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.g')))\n",
    "            \n",
    "            search_items = driver.find_elements(By.CSS_SELECTOR, 'div.g')\n",
    "            num_results += len(search_items)\n",
    "            collected_html += driver.page_source\n",
    "\n",
    "            if num_results >= 100:\n",
    "                break\n",
    "\n",
    "            # Try to find the \"Next\" button and click it\n",
    "            try:\n",
    "                next_button = wait.until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, 'a#pnnext'))\n",
    "                )\n",
    "                next_button.click()\n",
    "                time.sleep(2)  # Allow time for the page to load\n",
    "            except Exception:\n",
    "                try:\n",
    "                    more_results_button = wait.until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, '//a[contains(., \"More results\")]'))\n",
    "                    )\n",
    "                    more_results_button.click()\n",
    "                    time.sleep(2)  # Allow time for more results to load\n",
    "                except Exception:\n",
    "                    print(\"No more pages or unable to load more results.\")\n",
    "                    break  # No new items or next/more results button was not found\n",
    "        \n",
    "        driver.quit()\n",
    "        return collected_html # str: The HTML content of the loaded pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def save_html(self, html_content, base_filename=\"google_search_results.html\"): # html_content (str): The HTML content to save and base_filename (str): The base name of the file without the extension.\n",
    "        \"\"\"Save the HTML content to a timestamped file.\"\"\"\n",
    "        timestamped_file = timestamped_filename(base_filename)\n",
    "        with open(timestamped_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "        return timestamped_file # str: The timestamped filename of the saved HTML file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def save_results_to_json(self, results, base_filename=\"parsed_search_results.json\"): # results (list): The list of parsed results and base_filename (str): The base name of the file without the extension.\n",
    "        \"\"\"Save the parsed results to a timestamped JSON file.\"\"\"\n",
    "        json_filename = timestamped_filename(base_filename)\n",
    "        with open(json_filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "        return json_filename # str: The path to the saved JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def parse_items(self, html_content):\n",
    "        \"\"\"Parse search items from HTML content using BeautifulSoup.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser') # html_content (str): The HTML content to parse.\n",
    "        results = []\n",
    "        search_items = soup.select('div.g')  # Adjust the selector as needed\n",
    "        for item in search_items:\n",
    "            title_div = item.select_one('h3')\n",
    "            title = title_div.text if title_div else \"No title\"\n",
    "            link_tag = item.select_one('a')\n",
    "            link = link_tag['href'] if link_tag and 'href' in link_tag.attrs else None\n",
    "            snippet_div = item.select_one('div.kb0PBd')\n",
    "            snippet = snippet_div.text if snippet_div else \"No snippet available\"\n",
    "            if link:\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "        return results # list: A list of dictionaries containing parsed search items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_metadata(link): # link (str): The URL of the webpage.\n",
    "    \"\"\"Retrieve metadata (title, description, keywords) from a webpage.\"\"\"\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.get(link)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        title = driver.title\n",
    "        description = soup.find('meta', attrs={'name': 'description'})\n",
    "        keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
    "        \n",
    "        metadata = {\n",
    "            \"title\": title,\n",
    "            \"description\": description['content'] if description else \"No description\",\n",
    "            \"keywords\": keywords['content'] if keywords else \"No keywords\"\n",
    "        }\n",
    "        \n",
    "        driver.quit()\n",
    "        return metadata, None # tuple: A tuple containing the metadata dictionary and any error message.\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_link(link_info): # link_info (dict): A dictionary containing the link and optional error handling.\n",
    "    \"\"\"Test a single link by retrieving its metadata and returning a status.\"\"\"\n",
    "    link = link_info['link']\n",
    "    metadata, error = get_page_metadata(link)\n",
    "    if error:\n",
    "        return {'link': link, 'status': 'error', 'error': error}\n",
    "    else:\n",
    "        return {'link': link, 'status': 'success', **metadata} # dict: A dictionary with the link status and optionally error details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_links_concurrently(search_results): # search_results (list): A list of dictionaries representing links to test.\n",
    "    \"\"\"Test a list of links concurrently and collect the results.\"\"\"\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(test_link, item): item for item in search_results}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results # list: A list of dictionaries containing the test results for each link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(test_results, query_topic): # test_results (list): A list of dictionaries containing the test results and query_topic (str): The search query topic used for testing.\n",
    "    \"\"\"Generate a text-based report summarizing the test results.\"\"\"\n",
    "    success_count = sum(1 for result in test_results if result['status'] == 'success')\n",
    "    error_count = len(test_results) - success_count\n",
    "\n",
    "    report_lines = [\n",
    "        f\"Link Scrape Test Report for query: {query_topic}\",\n",
    "        f\"Total Links Tested: {len(test_results)}\",\n",
    "        f\"Total Successes: {success_count}\",\n",
    "        f\"Total Errors: {error_count}\",\n",
    "        \"\",\n",
    "        \"Details:\"\n",
    "    ]\n",
    "\n",
    "    for result in test_results:\n",
    "        if result['status'] == 'success':\n",
    "            report_lines.append(\n",
    "                f\"SUCCESS: {result['link']} - Title: {result['title']} - Description: {result['description']} - Keywords: {result['keywords']}\"\n",
    "            )\n",
    "        else:\n",
    "            report_lines.append(f\"ERROR: {result['link']} - Error: {result['error']}\")\n",
    "\n",
    "    return \"\\n\".join(report_lines) # str: A formatted string representing the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report_to_csv(test_results, query_topic): # test_results (list): A list of dictionaries containing the test results and query_topic (str): The search query topic used for testing.\n",
    "    \"\"\"Save the test results to a CSV file with a timestamped filename.\"\"\"\n",
    "    timestamped_csv = timestamped_filename(\"links_scrape_report.csv\")\n",
    "    with open(timestamped_csv, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['link', 'status', 'title', 'description', 'keywords', 'error']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for result in test_results:\n",
    "            writer.writerow(result)\n",
    "    \n",
    "    return timestamped_csv # str: The path to the saved CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to orchestrate the scraping and testing workflow using Streamlit.\"\"\"\n",
    "    st.title('Google Search Scrape Test')\n",
    "    query = st.text_input('Please enter the search query topic:')\n",
    "    \n",
    "    if query:\n",
    "        if st.button('Run Scrape Test'):\n",
    "            loader = GoogleSearchLoader(query=query)\n",
    "\n",
    "            # Load and scroll through the search results, then save the HTML content\n",
    "            html_content = loader.load_and_scroll()\n",
    "            html_filename = loader.save_html(html_content)\n",
    "            st.write(f\"HTML content saved to {html_filename}\")\n",
    "\n",
    "            # Parse the HTML content to extract search results\n",
    "            search_results = loader.parse_items(html_content)\n",
    "            st.write(f\"Extracted {len(search_results)} items.\")\n",
    "\n",
    "            # Ensure we have at least 100 unique links\n",
    "            if len(search_results) < 100:\n",
    "                st.write(f\"Collected only {len(search_results)} links, retrying to get more...\")\n",
    "                while len(search_results) < 100:\n",
    "                    html_content = loader.load_and_scroll()\n",
    "                    search_results.extend(loader.parse_items(html_content))\n",
    "                    search_results = list({item['link']: item for item in search_results if item['link']}.values())\n",
    "                    st.write(f\"Collected {len(search_results)} links so far...\")\n",
    "\n",
    "            # Save the parsed search results to a JSON file\n",
    "            json_filename = loader.save_results_to_json(search_results[:100])  # Ensure we have at most 100 links\n",
    "            st.write(f\"Parsed search results saved to {json_filename}\")\n",
    "\n",
    "            # Load the parsed search results from the JSON file\n",
    "            with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "                search_results = json.load(f)\n",
    "\n",
    "            # Test the links concurrently and save the results\n",
    "            test_results = test_links_concurrently(search_results)\n",
    "            output_filename = timestamped_filename('links_scrape_test.json')\n",
    "            with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(test_results, f, ensure_ascii=False, indent=4)\n",
    "            st.write(f\"Test results saved to {output_filename}\")\n",
    "\n",
    "            # Generate the report\n",
    "            report = generate_report(test_results, query)\n",
    "            report_filename = timestamped_filename('links_scrape_report.txt')\n",
    "            with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(report)\n",
    "            st.write(f\"Report saved to {report_filename}\")\n",
    "\n",
    "            # Save the report to a CSV file\n",
    "            csv_filename = save_report_to_csv(test_results, query)\n",
    "            st.write(f\"CSV report saved to {csv_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': \"The Old Farmer's Almanac - Weather, Gardening, Full Moon, Best Days, Astronomy, News\", 'description': 'The Old Farmer’s Almanac is your trusted source for long range weather forecasts, moon phases, full moon dates and times, gardening tips, sunrise and sunset times, Best Days, tide charts, home remedies, folklore, and more. All from the oldest continuously-published and best-selling farmers’ almanac in North America.', 'keywords': 'weather,gardening,recipes,cooking,calendar,moon,sun,astronomy,news'}\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import traceback\n",
    "\n",
    "def get_page_metadata_selenium(url):\n",
    "    \"\"\"\n",
    "    Retrieve metadata (title, description, keywords) from a webpage using Selenium.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL of the webpage.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the metadata (title, description, keywords), or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Setup Selenium WebDriver\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')  # Run headlessly\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Navigate to the URL\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the page to fully load\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "\n",
    "        # Extract title\n",
    "        title = driver.title\n",
    "\n",
    "        # Extract meta tags for description and keywords with improved error handling\n",
    "        description = \"\"\n",
    "        keywords = \"\"\n",
    "        try:\n",
    "            description_element = driver.find_element(By.XPATH, \"//meta[@name='description']\")\n",
    "            description = description_element.get_attribute('content') if description_element else \"No description found\"\n",
    "        except Exception:\n",
    "            print(\"Description meta tag not found.\")\n",
    "\n",
    "        try:\n",
    "            keywords_element = driver.find_element(By.XPATH, \"//meta[@name='keywords']\")\n",
    "            keywords = keywords_element.get_attribute('content') if keywords_element else \"No keywords found\"\n",
    "        except Exception:\n",
    "            print(\"Keywords meta tag not found.\")\n",
    "\n",
    "        metadata = {\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"keywords\": keywords\n",
    "        }\n",
    "\n",
    "        driver.quit()\n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        # Handle exceptions and return error message\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        driver.quit()\n",
    "        return {\"error\": f\"Failed to retrieve metadata for URL '{url}': {str(e)}\"}\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://almanac.com\"\n",
    "    metadata = get_page_metadata_selenium(url)\n",
    "    print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[id=\"nav\"]\"}\n",
      "  (Session info: chrome-headless-shell=126.0.6478.128); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0031C203+27395]\n",
      "\t(No symbol) [0x002B3E04]\n",
      "\t(No symbol) [0x001B1B7F]\n",
      "\t(No symbol) [0x001F2C65]\n",
      "\t(No symbol) [0x001F2D3B]\n",
      "\t(No symbol) [0x0022EC82]\n",
      "\t(No symbol) [0x002139E4]\n",
      "\t(No symbol) [0x0022CB24]\n",
      "\t(No symbol) [0x00213736]\n",
      "\t(No symbol) [0x001E7541]\n",
      "\t(No symbol) [0x001E80BD]\n",
      "\tGetHandleVerifier [0x005D3AB3+2876339]\n",
      "\tGetHandleVerifier [0x00627F7D+3221629]\n",
      "\tGetHandleVerifier [0x0039D674+556916]\n",
      "\tGetHandleVerifier [0x003A478C+585868]\n",
      "\t(No symbol) [0x002BCE44]\n",
      "\t(No symbol) [0x002B9858]\n",
      "\t(No symbol) [0x002B99F7]\n",
      "\t(No symbol) [0x002ABF4E]\n",
      "\tBaseThreadInitThunk [0x75CA7BA9+25]\n",
      "\tRtlInitializeExceptionChain [0x779CC10B+107]\n",
      "\tRtlClearBits [0x779CC08F+191]\n",
      "\t(No symbol) [0x00000000]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rmata\\AppData\\Local\\Temp\\ipykernel_28900\\878235044.py\", line 40, in get_page_metadata_selenium\n",
      "    content = driver.find_element(By.ID, \"nav\")\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 748, in find_element\n",
      "    return self.execute(Command.FIND_ELEMENT, {\"using\": by, \"value\": value})[\"value\"]\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 354, in execute\n",
      "    self.error_handler.check_response(response)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\", line 229, in check_response\n",
      "    raise exception_class(message, screen, stacktrace)\n",
      "selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[id=\"nav\"]\"}\n",
      "  (Session info: chrome-headless-shell=126.0.6478.128); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0031C203+27395]\n",
      "\t(No symbol) [0x002B3E04]\n",
      "\t(No symbol) [0x001B1B7F]\n",
      "\t(No symbol) [0x001F2C65]\n",
      "\t(No symbol) [0x001F2D3B]\n",
      "\t(No symbol) [0x0022EC82]\n",
      "\t(No symbol) [0x002139E4]\n",
      "\t(No symbol) [0x0022CB24]\n",
      "\t(No symbol) [0x00213736]\n",
      "\t(No symbol) [0x001E7541]\n",
      "\t(No symbol) [0x001E80BD]\n",
      "\tGetHandleVerifier [0x005D3AB3+2876339]\n",
      "\tGetHandleVerifier [0x00627F7D+3221629]\n",
      "\tGetHandleVerifier [0x0039D674+556916]\n",
      "\tGetHandleVerifier [0x003A478C+585868]\n",
      "\t(No symbol) [0x002BCE44]\n",
      "\t(No symbol) [0x002B9858]\n",
      "\t(No symbol) [0x002B99F7]\n",
      "\t(No symbol) [0x002ABF4E]\n",
      "\tBaseThreadInitThunk [0x75CA7BA9+25]\n",
      "\tRtlInitializeExceptionChain [0x779CC10B+107]\n",
      "\tRtlClearBits [0x779CC08F+191]\n",
      "\t(No symbol) [0x00000000]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Failed to retrieve metadata for URL \\'https://almanac.com\\': Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[id=\"nav\"]\"}\\n  (Session info: chrome-headless-shell=126.0.6478.128); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\\nStacktrace:\\n\\tGetHandleVerifier [0x0031C203+27395]\\n\\t(No symbol) [0x002B3E04]\\n\\t(No symbol) [0x001B1B7F]\\n\\t(No symbol) [0x001F2C65]\\n\\t(No symbol) [0x001F2D3B]\\n\\t(No symbol) [0x0022EC82]\\n\\t(No symbol) [0x002139E4]\\n\\t(No symbol) [0x0022CB24]\\n\\t(No symbol) [0x00213736]\\n\\t(No symbol) [0x001E7541]\\n\\t(No symbol) [0x001E80BD]\\n\\tGetHandleVerifier [0x005D3AB3+2876339]\\n\\tGetHandleVerifier [0x00627F7D+3221629]\\n\\tGetHandleVerifier [0x0039D674+556916]\\n\\tGetHandleVerifier [0x003A478C+585868]\\n\\t(No symbol) [0x002BCE44]\\n\\t(No symbol) [0x002B9858]\\n\\t(No symbol) [0x002B99F7]\\n\\t(No symbol) [0x002ABF4E]\\n\\tBaseThreadInitThunk [0x75CA7BA9+25]\\n\\tRtlInitializeExceptionChain [0x779CC10B+107]\\n\\tRtlClearBits [0x779CC08F+191]\\n\\t(No symbol) [0x00000000]\\n'}\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import traceback\n",
    "\n",
    "def get_page_metadata_selenium(url):\n",
    "    \"\"\"\n",
    "    Retrieve metadata (title, description, keywords) from a webpage using Selenium.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL of the webpage.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the metadata (title, description, keywords), or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Setup Selenium WebDriver\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')  # Run headlessly\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Navigate to the URL\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the page to fully load\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "\n",
    "        # Extract title\n",
    "        title = driver.title\n",
    "        #source = driver.page_source\n",
    "        source = \"\"\n",
    "        content = driver.find_element(By.ID, \"nav\")\n",
    "        driver.find_element(By.CLASS_NAME, \"class\")\n",
    "\n",
    "        # Extract meta tags for description and keywords with improved error handling\n",
    "        description = \"\"\n",
    "        keywords = \"\"\n",
    "        try:\n",
    "            description_element = driver.find_element(By.XPATH, \"//meta[@name='description']\")\n",
    "            description = description_element.get_attribute('content') if description_element else \"No description found\"\n",
    "        except Exception:\n",
    "            print(\"Description meta tag not found.\")\n",
    "\n",
    "        try:\n",
    "            keywords_element = driver.find_element(By.XPATH, \"//meta[@name='keywords']\")\n",
    "            keywords = keywords_element.get_attribute('content') if keywords_element else \"No keywords found\"\n",
    "        except Exception:\n",
    "            print(\"Keywords meta tag not found.\")\n",
    "\n",
    "        metadata = {\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"keywords\": keywords,\n",
    "            \"source\": source,\n",
    "            \"content\": content\n",
    "        }\n",
    "\n",
    "        driver.quit()\n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        # Handle exceptions and return error message\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        driver.quit()\n",
    "        return {\"error\": f\"Failed to retrieve metadata for URL '{url}': {str(e)}\"}\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://almanac.com\"\n",
    "    metadata = get_page_metadata_selenium(url)\n",
    "    print(metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape_tester_100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
