{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ')' does not match opening parenthesis '{' on line 12 (1585425745.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 14\u001b[1;36m\u001b[0m\n\u001b[1;33m    ) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m closing parenthesis ')' does not match opening parenthesis '{' on line 12\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#from langchain.loaders import BaseLoader\n",
    "from langchain_core.document_loaders.base import BaseBlobParser, BaseLoader\n",
    "#from langchain.document_loaders.base import BaseLoader\n",
    "\n",
    "class GoogleSearchLoader(BaseLoader):\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "    def load(self):\n",
    "        url = f\"https://www.google.com/search?q={self.query}\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    ") AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        return response.text\n",
    "    def parse(self, html):\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        results = []\n",
    "        for g in soup.find_all('div', class_='BNeawe vvjwJb AP7Wnd'):\n",
    "            title = g.get_text()\n",
    "            link = g.find_parent('a')['href']\n",
    "            snippet = g.find_next_sibling('div').get_text()\n",
    "            results.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'snippet': snippet\n",
    "            })\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = GoogleSearchLoader(query=\"Python web scraping\")\n",
    "    # check html from Google search\n",
    "    html = loader.load()\n",
    "    print(f\"type(html): {type(html)}\")\n",
    "    print(f\"len(html): {len(html)}\")\n",
    "    print(f\"html: {html}\\n\")\n",
    "\n",
    "    # check parsed_results\n",
    "    parsed_results = loader.parse(html)\n",
    "    print(f\"type(parsed_results): {type(parsed_results)}\")\n",
    "    print(f\"len(parsed_results): {len(parsed_results)}\")\n",
    "    print(f\"parsed_results: {parsed_results}\\n\")\n",
    "    for result in parsed_results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\"\"\"\n",
    "1. **Install necessary libraries:**\n",
    "   You need to install `requests`, `beautifulsoup4`, and `LangChain`.\n",
    "\n",
    "   ```bash\n",
    "   pip install requests beautifulsoup4 langchain\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Search Example\n",
    "\n",
    "### 1. **Set up the Google search loader:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#from langchain.loaders import BaseLoader\n",
    "from langchain_core.document_loaders.base import BaseBlobParser, BaseLoader\n",
    "#from langchain.document_loaders.base import BaseLoader\n",
    "\n",
    "class GoogleSearchLoader(BaseLoader):\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "    def load(self):\n",
    "        url = f\"https://www.google.com/search?q={self.query}\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64\n",
    ") AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        return response.text\n",
    "    def parse(self, html):\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        results = []\n",
    "        for g in soup.find_all('div', class_='BNeawe vvjwJb AP7Wnd'):\n",
    "            title = g.get_text()\n",
    "            link = g.find_parent('a')['href']\n",
    "            snippet = g.find_next_sibling('div').get_text()\n",
    "            results.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'snippet': snippet\n",
    "            })\n",
    "        return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = GoogleSearchLoader(query=\"Python web scraping\")\n",
    "    html = loader.load()\n",
    "    parsed_results = loader.parse(html)\n",
    "    for result in parsed_results:\n",
    "        print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Web Scraping with Python - Scrapy & Splash', 'link': 'https://duckduckgo.com/y.js?ad_domain=udemy.com&ad_provider=bingv7aa&ad_type=txad&rut=d5a436fd990a77f577646c62e983d2c6f68b04263a12b74c1016b87f8b969569&u3=https%3A%2F%2Fwww.bing.com%2Faclick%3Fld%3De8XsvhknPhRdDvxTimhqqBEDVUCUzQXueGTHszpp3dACKYoXpq4YwcGXmD2g0Bl3G3Z6Yhvm8kViVQBtzVaznJy0EVa4l5AK_rM3CSyKtL1uJpOGDbJR%2DTk3rOjn3S9uKGyqd0yyD1AIHLTqHDDmXxIZk%2D0pk_eg0rqEDcW4zys1Nu6XG_vYwATAbD6Q7Y15%2DtTDPkeQ%26u%3DaHR0cHMlM2ElMmYlMmZ3d3cudWRlbXkuY29tJTJmY291cnNlJTJmYWR2YW5jZWQtd2ViLXNjcmFwaW5nLXdpdGgtcHl0aG9uLXVzaW5nLXNjcmFweS1zcGxhc2glMmYlM2Z1dG1fc291cmNlJTNkYmluZyUyNnV0bV9tZWRpdW0lM2R1ZGVteWFkcyUyNnV0bV9jYW1wYWlnbiUzZEJHLVBST1MtVEVDSC1FTi1EU0FfY2MuQkUlMjZ1dG1fY29udGVudCUzZGRlYWw0NTg0JTI2dXRtX3Rlcm0lM2RfLl9hZ18xMjEzODYxMjUyNTg5MDQ2Xy5fYWRfXy5fa3dfZW4lMjUyMERldiUyNTIwaGlnaF9ucHNfLl9kZV9jXy5fZG1fXy5fcGxfXy5fdGlfZGF0LTIzMjc2NjYxMTYwNjkxNzglM2Fsb2MtMTkwXy5fbGlfMTAzNzkyXy5fcGRfXy5fJTI2bWF0Y2h0eXBlJTNkYiUyNm1zY2xraWQlM2QxNTkzNjZiYmM3MTkxZDhjYmRmMDAwY2YxMDc1OWM3Nw%26rlid%3D159366bbc7191d8cbdf000cf10759c77&vqd=4-71618825442901084976891269810909325836&iurl=%7B1%7DIG%3DAF0C890255154BA498709366B454E33A%26CID%3D1D0C27ABC4F66BDC2B57330EC5316AD3%26ID%3DDevEx%2C5065.1', 'snippet': 'Join Millions of Learners from Around the World Already Learning on Udemy! Learn Python Web Scraping at Your Own Pace. Start Today and Become an Expert in Days'}\n",
      "{'title': 'Web Scraping With Python - Real-World Practice Projects', 'link': 'https://duckduckgo.com/y.js?ad_domain=codecademy.com&ad_provider=bingv7aa&ad_type=txad&rut=bfd82c895b20e02d912634e555eb948c5217d917533983d9a7018d0b448bd41b&u3=https%3A%2F%2Fwww.bing.com%2Faclick%3Fld%3De84_Sb1ACo0kmsU5P6GbekMjVUCUy6pildw3EX%2DWVn5If8RMEJ0diJALG42MOBdx2J7OmsooOCAAXYCbmdqC_goeeKqm6JK5odhiBDTfJmZB0jTQX3ZmKETRag_4vg2Xp3_%2Ddb3vLnaBuJZ6IgYAk7_P4wsBxT8rguY6nOfeuNsIe_MGLV3DhUwWvRSzRkDq05kWoy0A%26u%3DaHR0cHMlM2ElMmYlMmZ3d3cuY29kZWNhZGVteS5jb20lMmZsZWFybiUyZmxlYXJuLXdlYi1zY3JhcGluZyUzZnV0bV9pZCUzZHRfa3dkLTc5MDk2NjM2MzA1MjAzJTNhbG9jLTQxMDAlM2FhZ18xMjY1NTM4NTA1NDM5MTA5JTNhY3BfMzcwMzE0NTA3JTNhbl9zJTNhZF9jJTI2bXNjbGtpZCUzZDg2Zjk2MjAzMGZiODE1YWYxYjc4MDQ3N2Q4ZDQxZTFhJTI2dXRtX3NvdXJjZSUzZGJpbmclMjZ1dG1fbWVkaXVtJTNkY3BjJTI2dXRtX2NhbXBhaWduJTNkVVMlMjUyMExhbmd1YWdlJTI1M0ElMjUyMFBybyUyNTIwLSUyNTIwRXhhY3QlMjZ1dG1fdGVybSUzZHdlYiUyNTIwc2NyYXBpbmclMjUyMHdpdGglMjUyMHB5dGhvbiUyNnV0bV9jb250ZW50JTNkYmVhdXRpZnVsJTI1MjBzb3Vw%26rlid%3D86f962030fb815af1b780477d8d41e1a&vqd=4-202506792541772059676694496225197425627&iurl=%7B1%7DIG%3DAF0C890255154BA498709366B454E33A%26CID%3D1D0C27ABC4F66BDC2B57330EC5316AD3%26ID%3DDevEx%2C5068.1', 'snippet': 'Take your skills to a new level and join millions that have learned Beautiful Soup. Master your language with lessons, quizzes, and projects designed for real-life scenarios.'}\n",
      "{'title': 'Beautiful Soup: Build a Web Scraper With Python - Real Python', 'link': 'https://realpython.com/beautiful-soup-web-scraper-python/', 'snippet': 'Learn how to use requests and Beautiful Soup libraries to scrape and parse data from the Web. Follow a step-by-step project to scrape fake Python job listings and extract relevant information.'}\n",
      "{'title': 'Python Web Scraping Using Beautiful Soup: A Step-by-Step Tutorial', 'link': 'https://www.askpython.com/resources/python-web-scraping-beautiful-soup', 'snippet': \"Do this to scrape a website with BeautifulSoup successfully: 1. Install Python and Create a Virtual Environment. Visit Python's official website and download the latest version based on your operating system (Linux, macOS, or Windows). Run the Python installer and follow the installation instructions.\"}\n",
      "{'title': 'Web Scraping Python Tutorial - How to Scrape Data From A Website', 'link': 'https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/', 'snippet': 'Learn how to use Python and BeautifulSoup to scrape data from a website in this hands-on classroom guide. Follow 7 labs to extract title, body, head, and other elements from HTML pages.'}\n",
      "{'title': 'A Practical Introduction to Web Scraping in Python', 'link': 'https://realpython.com/python-web-scraping-practical-introduction/', 'snippet': 'Learn how to collect and parse data from websites using Python tools like urllib, string methods, regular expressions, and HTML parsers. This tutorial covers web scraping basics, forms, and real-time interaction with websites.'}\n",
      "{'title': 'Python Web Scraping Tutorial - GeeksforGeeks', 'link': 'https://www.geeksforgeeks.org/python-web-scraping-tutorial/', 'snippet': 'Learn how to extract data from websites using Python libraries and modules such as requests, BeautifulSoup, Selenium, lxml, urllib, and PyAutoGUI. See examples, code snippets, and tips for web scraping with Python.'}\n",
      "{'title': 'Python Web Scraping: Full Tutorial With Examples (2024)', 'link': 'https://www.scrapingbee.com/blog/web-scraping-101-with-python/', 'snippet': 'Learn how to scrape web data with Python using various tools and techniques. This post covers the web fundamentals, HTTP requests, HTML parsing, and more.'}\n",
      "{'title': 'Web Scraping with Python: A Complete Step-by-Step Guide + Code', 'link': 'https://medium.com/geekculture/web-scraping-with-python-a-complete-step-by-step-guide-code-5174e52340ea', 'snippet': 'Learn how to perform web scraping with Python using various tools and libraries, such as Beautiful Soup, Scrapy, Selenium, and more. Follow the step-by-step guide and code examples to extract data from websites and store it in different formats.'}\n",
      "{'title': 'Python Web Scraping Tutorials - Real Python', 'link': 'https://realpython.com/tutorials/web-scraping/', 'snippet': 'Learn how to use Python to download and select structured data from the web with web scraping. Find tutorials on HTML parsing, HTTP requests, web spiders, databases, and more.'}\n",
      "{'title': 'Web Scraping using Python (and Beautiful Soup) | DataCamp', 'link': 'https://www.datacamp.com/tutorial/web-scraping-using-python', 'snippet': \"Learn how to extract, manipulate and visualize data from the web using Python's Beautiful Soup module. Follow a tutorial with a 10K race dataset and answer questions about runners' performance.\"}\n",
      "{'title': 'How To Scrape Web Pages with Beautiful Soup and Python 3', 'link': 'https://www.digitalocean.com/community/tutorials/how-to-scrape-web-pages-with-beautiful-soup-and-python-3', 'snippet': 'Learn how to use the Beautiful Soup module to scrape web pages and collect data from the National Gallery of Art website. Follow the steps to import libraries, collect and parse web pages, and write data to a CSV file.'}\n",
      "{'title': 'Python Web Scraping Tutorial: Step-By-Step [2024 Guide] - Oxylabs', 'link': 'https://oxylabs.io/blog/python-web-scraping', 'snippet': 'Learn how to use Python libraries like requests, Beautiful Soup, lxml, Selenium, and pandas to scrape data from the web. Follow the steps to prepare a Python environment, find HTML elements, save scraped data, and more.'}\n",
      "{'title': 'Learn Web Scraping with Beautiful Soup | Codecademy', 'link': 'https://www.codecademy.com/learn/learn-web-scraping', 'snippet': 'This course teaches you how to use the Python library Beautiful Soup to parse HTML and XML pages and extract data from websites. You will learn how to scrape tweets, images, ratings, and more with hands-on projects and quizzes.'}\n",
      "{'title': 'Python Web Scraping Tutorial - How to Scrape Data From Any Website with ...', 'link': 'https://www.freecodecamp.org/news/how-to-scrape-websites-with-python-2/', 'snippet': \"Learn how to scrape data from any website with Python using BeautifulSoup and Selenium. Follow the steps to inspect, extract, and save movie titles from IMDb's top 250 list.\"}\n",
      "{'title': 'How To Crawl A Web Page with Scrapy and Python 3', 'link': 'https://www.digitalocean.com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3', 'snippet': 'Web scraping, often called web crawling or web spidering, is the act of programmatically going over a collection of web pages and extracting data, and is a powerful tool for working with data on the web. ... Scrapy is one of the most popular and powerful Python scraping libraries; it takes a \"batteries included\" approach to scraping ...'}\n",
      "{'title': 'Web Scraping with Python - Beautiful Soup Crash Course', 'link': 'https://www.youtube.com/watch?v=XVv6mJpFOb0', 'snippet': 'Learn how to perform web scraping with Python using the Beautiful Soup library. ️ Tutorial by JimShapedCoding. Check out his YouTube Channel:https://www.yout...'}\n",
      "{'title': 'Build a Python web scraper with Beautiful Soup - LogRocket Blog', 'link': 'https://blog.logrocket.com/build-python-web-scraper-beautiful-soup/', 'snippet': 'Step 4: Extract the data with Beautiful Soup. Now that we have studied the structure of CoinGecko\\'s website, let\\'s use Beautiful Soup to extract the data we need. Add a new function to the scraper.py file: soup = BeautifulSoup(html, \"html.parser\") # find all the cryptocurrency elements.'}\n",
      "{'title': 'A guide to web scraping in Python using Beautiful Soup', 'link': 'https://opensource.com/article/21/9/web-scraping-python-beautiful-soup', 'snippet': 'Learn how to use the requests and Beautiful Soup libraries to extract HTML content from a website and convert it to a Python list or dictionary. Follow the steps and code examples to scrape the Technology section of this website.'}\n",
      "{'title': 'Ultimate Guide to Web Scraping with Python Part 1: Requests and ...', 'link': 'https://www.learndatasci.com/tutorials/ultimate-guide-web-scraping-w-python-requests-and-beautifulsoup/', 'snippet': 'Author: Brendan Martin Founder of LearnDataSci. Ultimate Guide to Web Scraping with Python Part 1: Requests and BeautifulSoup. Part one of this series focuses on requesting and wrangling HTML using two of the most popular Python libraries for web scraping: requests and BeautifulSoup. LearnDataSci is reader-supported.'}\n",
      "{'title': 'Tutorial: Web Scraping with Python Using Beautiful Soup - Dataquest', 'link': 'https://www.dataquest.io/blog/web-scraping-tutorial-python/', 'snippet': \"Web scraping is a technique that lets us use programming to do the heavy lifting. We'll write some code that looks at the NWS site, grabs just the data we want to work with, and outputs it in the format we need. In this tutorial, we'll show you how to perform web scraping using Python 3 and the Beautiful Soup library.\"}\n",
      "{'title': 'Scrapy | A Fast and Powerful Scraping and Web Crawling Framework', 'link': 'https://scrapy.org/', 'snippet': '{\" title \": \" Improved Frontera: Web Crawling at Scale with Python 3 Support \"} {\" title \": \" How to Crawl the Web Politely with Scrapy \"}... Deploy them to Zyte Scrapy Cloud. or use Scrapyd to host the spiders on your own server. Fast and powerful. write the rules to extract the data and let Scrapy do the rest.'}\n",
      "{'title': 'Implementing Web Scraping in Python with BeautifulSoup', 'link': 'https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/', 'snippet': 'Steps involved in web scraping: Send an HTTP request to the URL of the webpage you want to access. The server responds to the request by returning the HTML content of the webpage. For this task, we will use a third-party HTTP library for python-requests. Once we have accessed the HTML content, we are left with the task of parsing the data.'}\n",
      "{'title': 'Python Tutorial: Web Scraping with Scrapy (8 Code Examples) - Dataquest', 'link': 'https://www.dataquest.io/blog/web-scraping-with-scrapy/', 'snippet': \"In this Python tutorial, we'll go over web scraping using Scrapy — and we'll work through a sample e-commerce website scraping project. By 2025 the internet will grow to more than 175 zetabytes of data. Unfortunately, a large portion of it is unstructured and not machine-readable. This means that you can access the data through websites and ...\"}\n",
      "{'title': 'Web Scraping Target.com Using Python - ScrapeHero', 'link': 'https://www.scrapehero.com/web-scraping-target-com/', 'snippet': 'Python is excellent for web scraping Target.com and storing the results in JSON format. Use Python requests to get the HTML code and the packages BeautifulSoup, json, and re to extract the content. The code in this tutorial will work until Target.com changes the JSON structure or URLs. If that happens, you need to update the code.'}\n",
      "{'title': 'How to scrape data from arbitrary number of row listings using python ...', 'link': 'https://stackoverflow.com/questions/78639769/how-to-scrape-data-from-arbitrary-number-of-row-listings-using-python-selenium', 'snippet': \"This is super useful because you can try different selectors/different functions much quicker than re-running your script every time. (And this applies to any Python project, especially when working with something unfamiliar like web source code where you'll surely need to try different approaches) -\"}\n",
      "{'title': 'Web Scraping Financial Data Using Python - Nanonets', 'link': 'https://nanonets.com/blog/web-scraping-financial-data/', 'snippet': 'The web scraping process. The web scraping process follows a set of common principles across all tools and use cases. These principles stay the same for this entire web scraping process: Identify target URLs: Users need to manually select the URLs of websites that they want to extract data from and keep them ready to input into the web scraping ...'}\n",
      "{'title': 'Web Scraping With Beautiful Soup and Python', 'link': 'https://realpython.com/courses/web-scraping-beautiful-soup/', 'snippet': \"Martin Breuss 25 Lessons 1h 42m intermediate data-science tools web-scraping. The incredible amount of data on the Internet is a rich resource for any field of research or personal interest. To effectively harvest that data, you'll need to become skilled at web scraping. The Python libraries requests and Beautiful Soup are powerful tools for ...\"}\n",
      "{'title': 'Mimic Human Behavior in Python Web Scraping - LinkedIn', 'link': 'https://www.linkedin.com/advice/3/how-can-you-mimic-human-browsing-behavior-python-rftde', 'snippet': 'In web development, Python is a powerful tool for web scraping, which is the process of extracting data from websites. However, simple scraping scripts can often be detected and blocked by websites.'}\n",
      "{'title': 'Scraping Browser API - Automated Browser for Scraping - Bright Data', 'link': 'https://brightdata.com/products/scraping-browser', 'snippet': 'The term \"headless browser\" refers to a web browser without a graphical user interface. When used with a proxy, headless browsers can be used to scrape data, but they are easily detected by bot-protection software, making large-scale data scraping difficult. GUI browsers, like Scraping Browser (aka \"headfull\"), use a graphical user ...'}\n",
      "{'title': 'How to install Python on Windows, Linux, and macOS - XDA Developers', 'link': 'https://www.xda-developers.com/how-to-install-python/', 'snippet': \"Python 2.7 is often used for legacy projects, but if you're writing your own software, then you should install Python 3.x. To install on Windows or Mac, do the following:\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## DuckDuckGo Search Example\n",
    "\n",
    "### 1. **Set up the DuckDuckGo search loader:**\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#from langchain.loaders import BaseLoader\n",
    "from langchain_core.document_loaders.base import BaseBlobParser, BaseLoader\n",
    "\n",
    "class DuckDuckGoSearchLoader(BaseLoader):\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "    def load(self):\n",
    "        url = f\"https://duckduckgo.com/html/?q={self.query}\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        return response.text\n",
    "    def parse(self, html):\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        results = []\n",
    "        for result in soup.find_all('a', class_='result__a'):\n",
    "            title = result.get_text()\n",
    "            link = result['href']\n",
    "            snippet = result.find_next('a', class_='result__snippet').get_text() if result.find_next('a', class_='result__snippet') else ''\n",
    "            results.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'snippet': snippet\n",
    "            })\n",
    "        return results\n",
    "if __name__ == \"__main__\":\n",
    "    loader = DuckDuckGoSearchLoader(query=\"Python web scraping\")\n",
    "    html = loader.load()\n",
    "    parsed_results = loader.parse(html)\n",
    "    for result in parsed_results:\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bing Search Example\n",
    "\n",
    "### 1. **Set up the Bing search loader:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.document_loaders.base import BaseBlobParser, BaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BaseLoader in module langchain_core.document_loaders.base:\n",
      "\n",
      "class BaseLoader(abc.ABC)\n",
      " |  Interface for Document Loader.\n",
      " |\n",
      " |  Implementations should implement the lazy-loading method using generators\n",
      " |  to avoid loading all Documents into memory at once.\n",
      " |\n",
      " |  `load` is provided just for user convenience and should not be overridden.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      BaseLoader\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  async alazy_load(self) -> 'AsyncIterator[Document]'\n",
      " |      A lazy loader for Documents.\n",
      " |\n",
      " |  async aload(self) -> 'List[Document]'\n",
      " |      Load data into Document objects.\n",
      " |\n",
      " |  lazy_load(self) -> 'Iterator[Document]'\n",
      " |      A lazy loader for Documents.\n",
      " |\n",
      " |  load(self) -> 'List[Document]'\n",
      " |      Load data into Document objects.\n",
      " |\n",
      " |  load_and_split(self, text_splitter: 'Optional[TextSplitter]' = None) -> 'List[Document]'\n",
      " |      Load Documents and split into chunks. Chunks are returned as Documents.\n",
      " |\n",
      " |      Do not override this method. It should be considered to be deprecated!\n",
      " |\n",
      " |      Args:\n",
      " |          text_splitter: TextSplitter instance to use for splitting documents.\n",
      " |            Defaults to RecursiveCharacterTextSplitter.\n",
      " |\n",
      " |      Returns:\n",
      " |          List of Documents.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(BaseLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader: <__main__.BingSearchLoader object at 0x000001FEFC3BE6D0>\n",
      "type(loader): <class '__main__.BingSearchLoader'>\n",
      "\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\connection.py:652\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    650\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 652\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_and_match_hostname(\n\u001b[0;32m    653\u001b[0m     sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    654\u001b[0m     cert_reqs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_reqs,\n\u001b[0;32m    655\u001b[0m     ssl_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_version,\n\u001b[0;32m    656\u001b[0m     ssl_minimum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_minimum_version,\n\u001b[0;32m    657\u001b[0m     ssl_maximum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_maximum_version,\n\u001b[0;32m    658\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs,\n\u001b[0;32m    659\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir,\n\u001b[0;32m    660\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_data,\n\u001b[0;32m    661\u001b[0m     cert_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_file,\n\u001b[0;32m    662\u001b[0m     key_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_file,\n\u001b[0;32m    663\u001b[0m     key_password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_password,\n\u001b[0;32m    664\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname_rm_dot,\n\u001b[0;32m    665\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context,\n\u001b[0;32m    666\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[0;32m    667\u001b[0m     assert_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_hostname,\n\u001b[0;32m    668\u001b[0m     assert_fingerprint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_fingerprint,\n\u001b[0;32m    669\u001b[0m )\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\connection.py:805\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    803\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 805\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m ssl_wrap_socket(\n\u001b[0;32m    806\u001b[0m     sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    807\u001b[0m     keyfile\u001b[38;5;241m=\u001b[39mkey_file,\n\u001b[0;32m    808\u001b[0m     certfile\u001b[38;5;241m=\u001b[39mcert_file,\n\u001b[0;32m    809\u001b[0m     key_password\u001b[38;5;241m=\u001b[39mkey_password,\n\u001b[0;32m    810\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39mca_certs,\n\u001b[0;32m    811\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39mca_cert_dir,\n\u001b[0;32m    812\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39mca_cert_data,\n\u001b[0;32m    813\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    814\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m    815\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[0;32m    816\u001b[0m )\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\util\\ssl_.py:465\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\util\\ssl_.py:509\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock, server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msslsocket_class\u001b[38;5;241m.\u001b[39m_create(\n\u001b[0;32m    518\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    519\u001b[0m         server_side\u001b[38;5;241m=\u001b[39mserver_side,\n\u001b[0;32m    520\u001b[0m         do_handshake_on_connect\u001b[38;5;241m=\u001b[39mdo_handshake_on_connect,\n\u001b[0;32m    521\u001b[0m         suppress_ragged_eofs\u001b[38;5;241m=\u001b[39msuppress_ragged_eofs,\n\u001b[0;32m    522\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    523\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    524\u001b[0m         session\u001b[38;5;241m=\u001b[39msession\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\ssl.py:1108\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1108\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\ssl.py:1383\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1382\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    844\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    845\u001b[0m )\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\util\\retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m reraise(\u001b[38;5;28mtype\u001b[39m(error), error, _stacktrace)\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\connection.py:652\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    650\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 652\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_and_match_hostname(\n\u001b[0;32m    653\u001b[0m     sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    654\u001b[0m     cert_reqs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_reqs,\n\u001b[0;32m    655\u001b[0m     ssl_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_version,\n\u001b[0;32m    656\u001b[0m     ssl_minimum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_minimum_version,\n\u001b[0;32m    657\u001b[0m     ssl_maximum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_maximum_version,\n\u001b[0;32m    658\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs,\n\u001b[0;32m    659\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir,\n\u001b[0;32m    660\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_data,\n\u001b[0;32m    661\u001b[0m     cert_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_file,\n\u001b[0;32m    662\u001b[0m     key_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_file,\n\u001b[0;32m    663\u001b[0m     key_password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_password,\n\u001b[0;32m    664\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname_rm_dot,\n\u001b[0;32m    665\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context,\n\u001b[0;32m    666\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[0;32m    667\u001b[0m     assert_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_hostname,\n\u001b[0;32m    668\u001b[0m     assert_fingerprint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_fingerprint,\n\u001b[0;32m    669\u001b[0m )\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\connection.py:805\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    803\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 805\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m ssl_wrap_socket(\n\u001b[0;32m    806\u001b[0m     sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    807\u001b[0m     keyfile\u001b[38;5;241m=\u001b[39mkey_file,\n\u001b[0;32m    808\u001b[0m     certfile\u001b[38;5;241m=\u001b[39mcert_file,\n\u001b[0;32m    809\u001b[0m     key_password\u001b[38;5;241m=\u001b[39mkey_password,\n\u001b[0;32m    810\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39mca_certs,\n\u001b[0;32m    811\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39mca_cert_dir,\n\u001b[0;32m    812\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39mca_cert_data,\n\u001b[0;32m    813\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    814\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m    815\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[0;32m    816\u001b[0m )\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\util\\ssl_.py:465\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\urllib3\\util\\ssl_.py:509\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock, server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msslsocket_class\u001b[38;5;241m.\u001b[39m_create(\n\u001b[0;32m    518\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    519\u001b[0m         server_side\u001b[38;5;241m=\u001b[39mserver_side,\n\u001b[0;32m    520\u001b[0m         do_handshake_on_connect\u001b[38;5;241m=\u001b[39mdo_handshake_on_connect,\n\u001b[0;32m    521\u001b[0m         suppress_ragged_eofs\u001b[38;5;241m=\u001b[39msuppress_ragged_eofs,\n\u001b[0;32m    522\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    523\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    524\u001b[0m         session\u001b[38;5;241m=\u001b[39msession\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\ssl.py:1108\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1108\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\ssl.py:1383\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1382\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype(loader): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m() \n\u001b[1;32m---> 38\u001b[0m html \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mhtml: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhtml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m() \n",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m, in \u001b[0;36mBingSearchLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     13\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     15\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Delay for 1 second to mimic human behavior\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\requests\\adapters.py:682\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    686\u001b[0m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#from langchain.loaders import BaseLoader\n",
    "from langchain_core.document_loaders.base import BaseBlobParser, BaseLoader\n",
    "\n",
    "class BingSearchLoader(BaseLoader):\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def load(self):\n",
    "        url = f\"https://www.bing.com/search?q={self.query}\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "        time.sleep(1)  # Delay for 1 second to mimic human behavior\n",
    "        response = requests.get(url, headers=headers)\n",
    "        return response.text\n",
    "\n",
    "    def parse(self, html):\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        results = []\n",
    "        for result in soup.find_all('li', class_='b_algo'):\n",
    "            title = result.find('h2').get_text()\n",
    "            link = result.find('a')['href']\n",
    "            snippet = result.find('p').get_text() if result.find('p') else ''\n",
    "            results.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'snippet': snippet\n",
    "            })\n",
    "        return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = BingSearchLoader(query=\"Python web scraping\")\n",
    "    print(f\"loader: {loader}\")\n",
    "    print(f\"type(loader): {type(loader)}\")\n",
    "    print() \n",
    "    html = loader.load()\n",
    "    print(f\"\\nhtml: {html}\\n\")\n",
    "    print() \n",
    "    \n",
    "    parsed_results = loader.parse(html)\n",
    "    print(f\"parsed_results: {loader.parse(html)}\\n\")\n",
    "\n",
    "    for result in parsed_results:\n",
    "        print(result)\n",
    "\n",
    "#import webbrowser\n",
    "#webbrowser.open('html', new=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Example with LangChain Integration\n",
    "\n",
    "### To integrate this with LangChain, you can use the custom loaders created above. Here’s how to use one of them with LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader: <__main__.CustomLoader object at 0x000001FEFC3AEA50>\n",
      "type(loader): <class '__main__.CustomLoader'>\n",
      "\n",
      "length of parsed_results: 31\n",
      "\n",
      "parsed_results: [{'title': 'Scrapy & Splash Python - Web Scraping with Python', 'link': 'https://duckduckgo.com/y.js?ad_domain=udemy.com&ad_provider=bingv7aa&ad_type=txad&rut=47c8e919190d7bedcc7eb39908cbb04e5fabd177bd2111441544ccfd4ecfa09f&u3=https%3A%2F%2Fwww.bing.com%2Faclick%3Fld%3De82TQui1W3GxY3_rn9iQVSlzVUCUycTS_Fm8qn0iirf0%2DR3o04RQiJVUgUvvmKmZt55kPzrNj8AR4VFCHtv8inCvCRrGa8hkslS2JhcUpE50CJn5ZaVH5fNh%2DMX3T05IDVNkghISf%2D9iZ%2D7dVEto3Isll_pLyfu2Ziuu4HrW7mak8D9IHfRlauIm2Bl_QaycrBJR_73A%26u%3DaHR0cHMlM2ElMmYlMmZ3d3cudWRlbXkuY29tJTJmY291cnNlJTJmYWR2YW5jZWQtd2ViLXNjcmFwaW5nLXdpdGgtcHl0aG9uLXVzaW5nLXNjcmFweS1zcGxhc2glMmYlM2Z1dG1fc291cmNlJTNkYmluZyUyNnV0bV9tZWRpdW0lM2R1ZGVteWFkcyUyNnV0bV9jYW1wYWlnbiUzZEJHLVBST1MtVEVDSC1FTi1EU0FfY2MuQkUlMjZ1dG1fY29udGVudCUzZGRlYWw0NTg0JTI2dXRtX3Rlcm0lM2RfLl9hZ18xMjEzODYxMjUyNTg5MDQ2Xy5fYWRfXy5fa3dfZW4lMjUyMERldiUyNTIwaGlnaF9ucHNfLl9kZV9jXy5fZG1fXy5fcGxfXy5fdGlfZGF0LTIzMjc2NjYxMTYwNjkxNzglM2Fsb2MtMTkwXy5fbGlfMTAzNzkyXy5fcGRfXy5fJTI2bWF0Y2h0eXBlJTNkYiUyNm1zY2xraWQlM2Q4NDRlMGYxNDI1NDMxMGIyMDgyMjY5NjBmZDYxYWIxNA%26rlid%3D844e0f14254310b208226960fd61ab14&vqd=4-36574528002193618016657349168758476010&iurl=%7B1%7DIG%3DAA3286EA1B004669BA0A360362F3AA48%26CID%3D1490209697FE661B0BAE343396B167CC%26ID%3DDevEx%2C5063.1', 'snippet': 'Join Millions of Learners from Around the World Already Learning on Udemy! Learn Python Web Scraping at Your Own Pace. Start Today and Become an Expert in Days'}, {'title': 'Web Scraping With Python - Real-World Practice Projects', 'link': 'https://duckduckgo.com/y.js?ad_domain=codecademy.com&ad_provider=bingv7aa&ad_type=txad&rut=4839ad9d75f4522cbff9545686557702f341f0ef1b858f5014c086a44b8b6a90&u3=https%3A%2F%2Fwww.bing.com%2Faclick%3Fld%3De8KCqje1sdWJahU7%2DnmwbIpTVUCUzKu2pW5pDugUYbQKRVRCkHigpObokkRAKTEvjiVryd1loI3Du0ZOzWG7V6Z9hMZTH2uC_%2DKMwCXcbif6reNvbe3IUI9mKhBKlCmnWjMeuzX8dnXZF2wVK8XppX6RB4SyHecg2deA9QMaE8XQFvbVNPcR7fn5Q3WmfTszpu_86oeg%26u%3DaHR0cHMlM2ElMmYlMmZ3d3cuY29kZWNhZGVteS5jb20lMmZsZWFybiUyZmxlYXJuLXdlYi1zY3JhcGluZyUzZnV0bV9pZCUzZHRfa3dkLTc5MDk2NjM2MzA1MjAzJTNhbG9jLTQxMDAlM2FhZ18xMjY1NTM4NTA1NDM5MTA5JTNhY3BfMzcwMzE0NTA3JTNhbl9zJTNhZF9jJTI2bXNjbGtpZCUzZDkzZTRiZDFlOGYwNTEyYTExYjFhZGI2YTU3MjczZDgzJTI2dXRtX3NvdXJjZSUzZGJpbmclMjZ1dG1fbWVkaXVtJTNkY3BjJTI2dXRtX2NhbXBhaWduJTNkVVMlMjUyMExhbmd1YWdlJTI1M0ElMjUyMFBybyUyNTIwLSUyNTIwRXhhY3QlMjZ1dG1fdGVybSUzZHdlYiUyNTIwc2NyYXBpbmclMjUyMHdpdGglMjUyMHB5dGhvbiUyNnV0bV9jb250ZW50JTNkYmVhdXRpZnVsJTI1MjBzb3Vw%26rlid%3D93e4bd1e8f0512a11b1adb6a57273d83&vqd=4-338192803211875853525676640856687452316&iurl=%7B1%7DIG%3DAA3286EA1B004669BA0A360362F3AA48%26CID%3D1490209697FE661B0BAE343396B167CC%26ID%3DDevEx%2C5066.1', 'snippet': 'Take your skills to a new level and join millions that have learned Beautiful Soup. Master your language with lessons, quizzes, and projects designed for real-life scenarios.'}, {'title': 'Beautiful Soup: Build a Web Scraper With Python - Real Python', 'link': 'https://realpython.com/beautiful-soup-web-scraper-python/', 'snippet': 'Learn how to use requests and Beautiful Soup libraries to scrape and parse data from the Web. Follow a step-by-step project to scrape fake Python job listings and extract relevant information.'}, {'title': 'A Practical Introduction to Web Scraping in Python', 'link': 'https://realpython.com/python-web-scraping-practical-introduction/', 'snippet': 'Learn how to collect and parse data from websites using Python tools like urllib, string methods, regular expressions, and HTML parsers. This tutorial covers web scraping basics, forms, and real-time interaction with websites.'}, {'title': 'Web Scraping Python Tutorial - How to Scrape Data From A Website', 'link': 'https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/', 'snippet': 'Learn how to use Python and BeautifulSoup to scrape data from a website in this hands-on classroom guide. Follow 7 labs to extract title, body, head, and other elements from HTML pages.'}, {'title': 'Python Web Scraping Using Beautiful Soup: A Step-by-Step Tutorial', 'link': 'https://www.askpython.com/resources/python-web-scraping-beautiful-soup', 'snippet': \"Do this to scrape a website with BeautifulSoup successfully: 1. Install Python and Create a Virtual Environment. Visit Python's official website and download the latest version based on your operating system (Linux, macOS, or Windows). Run the Python installer and follow the installation instructions.\"}, {'title': 'Python Web Scraping Tutorial - GeeksforGeeks', 'link': 'https://www.geeksforgeeks.org/python-web-scraping-tutorial/', 'snippet': 'Learn how to extract data from websites using Python libraries and modules such as requests, BeautifulSoup, Selenium, lxml, urllib, and PyAutoGUI. See examples, code snippets, and tips for web scraping with Python.'}, {'title': 'Python Web Scraping Tutorials - Real Python', 'link': 'https://realpython.com/tutorials/web-scraping/', 'snippet': 'Learn how to use Python to download and select structured data from the web with web scraping. Find tutorials on HTML parsing, HTTP requests, web spiders, databases, and more.'}, {'title': 'Web Scraping using Python (and Beautiful Soup) | DataCamp', 'link': 'https://www.datacamp.com/tutorial/web-scraping-using-python', 'snippet': \"Learn how to extract, manipulate and visualize data from the web using Python's Beautiful Soup module. Follow a tutorial with a 10K race dataset and answer questions about runners' performance.\"}, {'title': 'Python Web Scraping: Full Tutorial With Examples (2024)', 'link': 'https://www.scrapingbee.com/blog/web-scraping-101-with-python/', 'snippet': 'Learn how to scrape web data with Python using various tools and techniques. This post covers the web fundamentals, HTTP requests, HTML parsing, and more.'}, {'title': 'Python Web Scraping Tutorial - How to Scrape Data From Any Website with ...', 'link': 'https://www.freecodecamp.org/news/how-to-scrape-websites-with-python-2/', 'snippet': \"Learn how to scrape data from any website with Python using BeautifulSoup and Selenium. Follow the steps to inspect, extract, and save movie titles from IMDb's top 250 list.\"}, {'title': 'Python Web Scraping Tutorial: Step-By-Step [2024 Guide] - Oxylabs', 'link': 'https://oxylabs.io/blog/python-web-scraping', 'snippet': 'Learn how to use Python libraries like requests, Beautiful Soup, lxml, Selenium, and pandas to scrape data from the web. Follow the steps to prepare a Python environment, find HTML elements, save scraped data, and more.'}, {'title': 'A guide to web scraping in Python using Beautiful Soup', 'link': 'https://opensource.com/article/21/9/web-scraping-python-beautiful-soup', 'snippet': 'Learn how to use the requests and Beautiful Soup libraries to extract HTML content from a website and convert it to a Python list or dictionary. Follow the steps and code examples to scrape the Technology section of this website.'}, {'title': 'Python Tutorial: Web Scraping with Scrapy (8 Code Examples) - Dataquest', 'link': 'https://www.dataquest.io/blog/web-scraping-with-scrapy/', 'snippet': \"In this Python tutorial, we'll go over web scraping using Scrapy — and we'll work through a sample e-commerce website scraping project. By 2025 the internet will grow to more than 175 zetabytes of data. Unfortunately, a large portion of it is unstructured and not machine-readable. This means that you can access the data through websites and ...\"}, {'title': 'Implementing Web Scraping in Python with BeautifulSoup', 'link': 'https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/', 'snippet': 'Steps involved in web scraping: Send an HTTP request to the URL of the webpage you want to access. The server responds to the request by returning the HTML content of the webpage. For this task, we will use a third-party HTTP library for python-requests. Once we have accessed the HTML content, we are left with the task of parsing the data.'}, {'title': 'Web Scraping with Python (A Comprehensive Guide)', 'link': 'https://www.scrapingdog.com/blog/web-scraping-with-python/', 'snippet': 'Learn how to scrape data from any website using Python libraries such as requests, BS4, pandas, and Scrapy. See examples of scraping Amazon, Google, Yelp, and more with code and tips.'}, {'title': 'Web scraping with Python: A quick guide - Educative', 'link': 'https://www.educative.io/blog/python-web-scraping-tutorial', 'snippet': 'Overview: Web scraping with Python. Build a web scraper with Python. Step 1: Select the URLs you want to scrape. Step 2: Find the HTML content you want to scrape. Step 3: Choose your tools and libraries. Step 4: Build your web scraper in Python. Completed code. Step 5: Repeat for Madewell. Wrapping up and next steps.'}, {'title': 'Web Scraping With Python Libraries | LearnPython.com', 'link': 'https://learnpython.com/blog/python-web-scraping/', 'snippet': 'Web scraping is the process of extracting information from the source code of a web page. This may be text, numerical data, or even images. It is the first step for many interesting projects! However, there is no fixed technology or methodology for Python web scraping. The best approach is very use-case dependent.'}, {'title': 'Web Scraping Using Python: Step-by-Step Guide - Geekflare', 'link': 'https://geekflare.com/web-scraping-with-python/', 'snippet': 'Web scraping using Python is a very popular choice as Python provides multiple libraries like BeautifulSoup, or Scrapy to extract data effectively. Having the skill of extracting data efficiently is also very important as a developer or a data scientist.'}, {'title': 'Web Scraping With Python - 2024 Full Guide - Bright Data', 'link': 'https://brightdata.com/blog/how-tos/web-scraping-with-python', 'snippet': 'Web scraping is about extracting data from the Web. Specifically, a web scraper is a tool that can perform web scraping. Python is one of the easiest scripting languages available and comes with a wide variety of web scraping libraries. This makes it the perfect programming language for web scraping. Python web scraping takes only a few lines ...'}, {'title': 'Web Scraping with Python - ScrapFly Blog', 'link': 'https://scrapfly.io/blog/web-scraping-with-python/', 'snippet': \"In this python web scraping tutorial, we've covered everything you need to know to start web scraping in Python. We've introduced ourselves with the HTTP protocol which is the backbone of all internet connections. We explored GET and POST requests, and the importance of request headers for avoiding blocking.\"}, {'title': 'Web Scraping With Python - Full Guide to Python Web Scraping - Edureka', 'link': 'https://www.edureka.co/blog/web-scraping-with-python/', 'snippet': 'The code then, parses the HTML or XML page, finds the data and extracts it. To extract data using web scraping with python, you need to follow these basic steps: Find the URL that you want to scrape. Inspecting the Page. Find the data you want to extract. Write the code. Run the code and extract the data.'}, {'title': 'Web Scraping & NLP in Python | DataCamp', 'link': 'https://www.datacamp.com/tutorial/web-scraping-python-nlp', 'snippet': \"We won't give you the novels: you'll learn to scrape them from the website Project Gutenberg (which basically contains a large corpus of books) using the Python package requests and how to extract the novels from this web data using BeautifulSoup.Then you'll dive in to analyzing the novels using the Natural Language ToolKit (nltk).In the process, you'll learn about important aspects of Natural ...\"}, {'title': 'How to Web Scrape with Python in 4 Minutes | by Julia Kho | Towards ...', 'link': 'https://towardsdatascience.com/how-to-web-scrape-with-python-in-4-minutes-bc49186a8460', 'snippet': 'It is important to understand the basics of HTML in order to successfully web scrape. On the website, right click and click on \"Inspect\". This allows you to see the raw code behind the site. Once you\\'ve clicked on \"Inspect\", you should see this console pop up. Console.'}, {'title': 'Web Scraping With Python - Serpdog', 'link': 'https://serpdog.io/blog/web-scraping-with-python/', 'snippet': \"Web Scraping is the process of extracting a specific set of information from websites in the form of text, videos, images, and links. In today's world, web scraping is an important skill to learn, as it can be used for a variety of purposes, such as lead generation, price monitoring, SERP monitoring, etc. Web Scraping With Python — A ...\"}, {'title': 'How to scrape data from arbitrary number of row listings using python ...', 'link': 'https://stackoverflow.com/questions/78639769/how-to-scrape-data-from-arbitrary-number-of-row-listings-using-python-selenium', 'snippet': \"This is super useful because you can try different selectors/different functions much quicker than re-running your script every time. (And this applies to any Python project, especially when working with something unfamiliar like web source code where you'll surely need to try different approaches) -\"}, {'title': 'Web Scraping Financial Data Using Python - Nanonets', 'link': 'https://nanonets.com/blog/web-scraping-financial-data/', 'snippet': 'The web scraping process. The web scraping process follows a set of common principles across all tools and use cases. These principles stay the same for this entire web scraping process: Identify target URLs: Users need to manually select the URLs of websites that they want to extract data from and keep them ready to input into the web scraping ...'}, {'title': 'Web Scraping Target.com Using Python - ScrapeHero', 'link': 'https://www.scrapehero.com/web-scraping-target-com/', 'snippet': 'Python is excellent for web scraping Target.com and storing the results in JSON format. Use Python requests to get the HTML code and the packages BeautifulSoup, json, and re to extract the content. The code in this tutorial will work until Target.com changes the JSON structure or URLs. If that happens, you need to update the code.'}, {'title': 'Scraping Browser API - Automated Browser for Scraping - Bright Data', 'link': 'https://brightdata.com/products/scraping-browser', 'snippet': 'The term \"headless browser\" refers to a web browser without a graphical user interface. When used with a proxy, headless browsers can be used to scrape data, but they are easily detected by bot-protection software, making large-scale data scraping difficult. GUI browsers, like Scraping Browser (aka \"headfull\"), use a graphical user ...'}, {'title': 'Mimic Human Behavior in Python Web Scraping - LinkedIn', 'link': 'https://www.linkedin.com/advice/3/how-can-you-mimic-human-browsing-behavior-python-rftde', 'snippet': 'In web development, Python is a powerful tool for web scraping, which is the process of extracting data from websites. However, simple scraping scripts can often be detected and blocked by websites.'}, {'title': 'How to install Python on Windows, Linux, and macOS - XDA Developers', 'link': 'https://www.xda-developers.com/how-to-install-python/', 'snippet': \"Python 2.7 is often used for legacy projects, but if you're writing your own software, then you should install Python 3.x. To install on Windows or Mac, do the following:\"}]\n",
      "\n",
      "{'title': 'Scrapy & Splash Python - Web Scraping with Python', 'link': 'https://duckduckgo.com/y.js?ad_domain=udemy.com&ad_provider=bingv7aa&ad_type=txad&rut=47c8e919190d7bedcc7eb39908cbb04e5fabd177bd2111441544ccfd4ecfa09f&u3=https%3A%2F%2Fwww.bing.com%2Faclick%3Fld%3De82TQui1W3GxY3_rn9iQVSlzVUCUycTS_Fm8qn0iirf0%2DR3o04RQiJVUgUvvmKmZt55kPzrNj8AR4VFCHtv8inCvCRrGa8hkslS2JhcUpE50CJn5ZaVH5fNh%2DMX3T05IDVNkghISf%2D9iZ%2D7dVEto3Isll_pLyfu2Ziuu4HrW7mak8D9IHfRlauIm2Bl_QaycrBJR_73A%26u%3DaHR0cHMlM2ElMmYlMmZ3d3cudWRlbXkuY29tJTJmY291cnNlJTJmYWR2YW5jZWQtd2ViLXNjcmFwaW5nLXdpdGgtcHl0aG9uLXVzaW5nLXNjcmFweS1zcGxhc2glMmYlM2Z1dG1fc291cmNlJTNkYmluZyUyNnV0bV9tZWRpdW0lM2R1ZGVteWFkcyUyNnV0bV9jYW1wYWlnbiUzZEJHLVBST1MtVEVDSC1FTi1EU0FfY2MuQkUlMjZ1dG1fY29udGVudCUzZGRlYWw0NTg0JTI2dXRtX3Rlcm0lM2RfLl9hZ18xMjEzODYxMjUyNTg5MDQ2Xy5fYWRfXy5fa3dfZW4lMjUyMERldiUyNTIwaGlnaF9ucHNfLl9kZV9jXy5fZG1fXy5fcGxfXy5fdGlfZGF0LTIzMjc2NjYxMTYwNjkxNzglM2Fsb2MtMTkwXy5fbGlfMTAzNzkyXy5fcGRfXy5fJTI2bWF0Y2h0eXBlJTNkYiUyNm1zY2xraWQlM2Q4NDRlMGYxNDI1NDMxMGIyMDgyMjY5NjBmZDYxYWIxNA%26rlid%3D844e0f14254310b208226960fd61ab14&vqd=4-36574528002193618016657349168758476010&iurl=%7B1%7DIG%3DAA3286EA1B004669BA0A360362F3AA48%26CID%3D1490209697FE661B0BAE343396B167CC%26ID%3DDevEx%2C5063.1', 'snippet': 'Join Millions of Learners from Around the World Already Learning on Udemy! Learn Python Web Scraping at Your Own Pace. Start Today and Become an Expert in Days'}\n",
      "{'title': 'Web Scraping With Python - Real-World Practice Projects', 'link': 'https://duckduckgo.com/y.js?ad_domain=codecademy.com&ad_provider=bingv7aa&ad_type=txad&rut=4839ad9d75f4522cbff9545686557702f341f0ef1b858f5014c086a44b8b6a90&u3=https%3A%2F%2Fwww.bing.com%2Faclick%3Fld%3De8KCqje1sdWJahU7%2DnmwbIpTVUCUzKu2pW5pDugUYbQKRVRCkHigpObokkRAKTEvjiVryd1loI3Du0ZOzWG7V6Z9hMZTH2uC_%2DKMwCXcbif6reNvbe3IUI9mKhBKlCmnWjMeuzX8dnXZF2wVK8XppX6RB4SyHecg2deA9QMaE8XQFvbVNPcR7fn5Q3WmfTszpu_86oeg%26u%3DaHR0cHMlM2ElMmYlMmZ3d3cuY29kZWNhZGVteS5jb20lMmZsZWFybiUyZmxlYXJuLXdlYi1zY3JhcGluZyUzZnV0bV9pZCUzZHRfa3dkLTc5MDk2NjM2MzA1MjAzJTNhbG9jLTQxMDAlM2FhZ18xMjY1NTM4NTA1NDM5MTA5JTNhY3BfMzcwMzE0NTA3JTNhbl9zJTNhZF9jJTI2bXNjbGtpZCUzZDkzZTRiZDFlOGYwNTEyYTExYjFhZGI2YTU3MjczZDgzJTI2dXRtX3NvdXJjZSUzZGJpbmclMjZ1dG1fbWVkaXVtJTNkY3BjJTI2dXRtX2NhbXBhaWduJTNkVVMlMjUyMExhbmd1YWdlJTI1M0ElMjUyMFBybyUyNTIwLSUyNTIwRXhhY3QlMjZ1dG1fdGVybSUzZHdlYiUyNTIwc2NyYXBpbmclMjUyMHdpdGglMjUyMHB5dGhvbiUyNnV0bV9jb250ZW50JTNkYmVhdXRpZnVsJTI1MjBzb3Vw%26rlid%3D93e4bd1e8f0512a11b1adb6a57273d83&vqd=4-338192803211875853525676640856687452316&iurl=%7B1%7DIG%3DAA3286EA1B004669BA0A360362F3AA48%26CID%3D1490209697FE661B0BAE343396B167CC%26ID%3DDevEx%2C5066.1', 'snippet': 'Take your skills to a new level and join millions that have learned Beautiful Soup. Master your language with lessons, quizzes, and projects designed for real-life scenarios.'}\n",
      "{'title': 'Beautiful Soup: Build a Web Scraper With Python - Real Python', 'link': 'https://realpython.com/beautiful-soup-web-scraper-python/', 'snippet': 'Learn how to use requests and Beautiful Soup libraries to scrape and parse data from the Web. Follow a step-by-step project to scrape fake Python job listings and extract relevant information.'}\n",
      "{'title': 'A Practical Introduction to Web Scraping in Python', 'link': 'https://realpython.com/python-web-scraping-practical-introduction/', 'snippet': 'Learn how to collect and parse data from websites using Python tools like urllib, string methods, regular expressions, and HTML parsers. This tutorial covers web scraping basics, forms, and real-time interaction with websites.'}\n",
      "{'title': 'Web Scraping Python Tutorial - How to Scrape Data From A Website', 'link': 'https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/', 'snippet': 'Learn how to use Python and BeautifulSoup to scrape data from a website in this hands-on classroom guide. Follow 7 labs to extract title, body, head, and other elements from HTML pages.'}\n",
      "{'title': 'Python Web Scraping Using Beautiful Soup: A Step-by-Step Tutorial', 'link': 'https://www.askpython.com/resources/python-web-scraping-beautiful-soup', 'snippet': \"Do this to scrape a website with BeautifulSoup successfully: 1. Install Python and Create a Virtual Environment. Visit Python's official website and download the latest version based on your operating system (Linux, macOS, or Windows). Run the Python installer and follow the installation instructions.\"}\n",
      "{'title': 'Python Web Scraping Tutorial - GeeksforGeeks', 'link': 'https://www.geeksforgeeks.org/python-web-scraping-tutorial/', 'snippet': 'Learn how to extract data from websites using Python libraries and modules such as requests, BeautifulSoup, Selenium, lxml, urllib, and PyAutoGUI. See examples, code snippets, and tips for web scraping with Python.'}\n",
      "{'title': 'Python Web Scraping Tutorials - Real Python', 'link': 'https://realpython.com/tutorials/web-scraping/', 'snippet': 'Learn how to use Python to download and select structured data from the web with web scraping. Find tutorials on HTML parsing, HTTP requests, web spiders, databases, and more.'}\n",
      "{'title': 'Web Scraping using Python (and Beautiful Soup) | DataCamp', 'link': 'https://www.datacamp.com/tutorial/web-scraping-using-python', 'snippet': \"Learn how to extract, manipulate and visualize data from the web using Python's Beautiful Soup module. Follow a tutorial with a 10K race dataset and answer questions about runners' performance.\"}\n",
      "{'title': 'Python Web Scraping: Full Tutorial With Examples (2024)', 'link': 'https://www.scrapingbee.com/blog/web-scraping-101-with-python/', 'snippet': 'Learn how to scrape web data with Python using various tools and techniques. This post covers the web fundamentals, HTTP requests, HTML parsing, and more.'}\n",
      "{'title': 'Python Web Scraping Tutorial - How to Scrape Data From Any Website with ...', 'link': 'https://www.freecodecamp.org/news/how-to-scrape-websites-with-python-2/', 'snippet': \"Learn how to scrape data from any website with Python using BeautifulSoup and Selenium. Follow the steps to inspect, extract, and save movie titles from IMDb's top 250 list.\"}\n",
      "{'title': 'Python Web Scraping Tutorial: Step-By-Step [2024 Guide] - Oxylabs', 'link': 'https://oxylabs.io/blog/python-web-scraping', 'snippet': 'Learn how to use Python libraries like requests, Beautiful Soup, lxml, Selenium, and pandas to scrape data from the web. Follow the steps to prepare a Python environment, find HTML elements, save scraped data, and more.'}\n",
      "{'title': 'A guide to web scraping in Python using Beautiful Soup', 'link': 'https://opensource.com/article/21/9/web-scraping-python-beautiful-soup', 'snippet': 'Learn how to use the requests and Beautiful Soup libraries to extract HTML content from a website and convert it to a Python list or dictionary. Follow the steps and code examples to scrape the Technology section of this website.'}\n",
      "{'title': 'Python Tutorial: Web Scraping with Scrapy (8 Code Examples) - Dataquest', 'link': 'https://www.dataquest.io/blog/web-scraping-with-scrapy/', 'snippet': \"In this Python tutorial, we'll go over web scraping using Scrapy — and we'll work through a sample e-commerce website scraping project. By 2025 the internet will grow to more than 175 zetabytes of data. Unfortunately, a large portion of it is unstructured and not machine-readable. This means that you can access the data through websites and ...\"}\n",
      "{'title': 'Implementing Web Scraping in Python with BeautifulSoup', 'link': 'https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/', 'snippet': 'Steps involved in web scraping: Send an HTTP request to the URL of the webpage you want to access. The server responds to the request by returning the HTML content of the webpage. For this task, we will use a third-party HTTP library for python-requests. Once we have accessed the HTML content, we are left with the task of parsing the data.'}\n",
      "{'title': 'Web Scraping with Python (A Comprehensive Guide)', 'link': 'https://www.scrapingdog.com/blog/web-scraping-with-python/', 'snippet': 'Learn how to scrape data from any website using Python libraries such as requests, BS4, pandas, and Scrapy. See examples of scraping Amazon, Google, Yelp, and more with code and tips.'}\n",
      "{'title': 'Web scraping with Python: A quick guide - Educative', 'link': 'https://www.educative.io/blog/python-web-scraping-tutorial', 'snippet': 'Overview: Web scraping with Python. Build a web scraper with Python. Step 1: Select the URLs you want to scrape. Step 2: Find the HTML content you want to scrape. Step 3: Choose your tools and libraries. Step 4: Build your web scraper in Python. Completed code. Step 5: Repeat for Madewell. Wrapping up and next steps.'}\n",
      "{'title': 'Web Scraping With Python Libraries | LearnPython.com', 'link': 'https://learnpython.com/blog/python-web-scraping/', 'snippet': 'Web scraping is the process of extracting information from the source code of a web page. This may be text, numerical data, or even images. It is the first step for many interesting projects! However, there is no fixed technology or methodology for Python web scraping. The best approach is very use-case dependent.'}\n",
      "{'title': 'Web Scraping Using Python: Step-by-Step Guide - Geekflare', 'link': 'https://geekflare.com/web-scraping-with-python/', 'snippet': 'Web scraping using Python is a very popular choice as Python provides multiple libraries like BeautifulSoup, or Scrapy to extract data effectively. Having the skill of extracting data efficiently is also very important as a developer or a data scientist.'}\n",
      "{'title': 'Web Scraping With Python - 2024 Full Guide - Bright Data', 'link': 'https://brightdata.com/blog/how-tos/web-scraping-with-python', 'snippet': 'Web scraping is about extracting data from the Web. Specifically, a web scraper is a tool that can perform web scraping. Python is one of the easiest scripting languages available and comes with a wide variety of web scraping libraries. This makes it the perfect programming language for web scraping. Python web scraping takes only a few lines ...'}\n",
      "{'title': 'Web Scraping with Python - ScrapFly Blog', 'link': 'https://scrapfly.io/blog/web-scraping-with-python/', 'snippet': \"In this python web scraping tutorial, we've covered everything you need to know to start web scraping in Python. We've introduced ourselves with the HTTP protocol which is the backbone of all internet connections. We explored GET and POST requests, and the importance of request headers for avoiding blocking.\"}\n",
      "{'title': 'Web Scraping With Python - Full Guide to Python Web Scraping - Edureka', 'link': 'https://www.edureka.co/blog/web-scraping-with-python/', 'snippet': 'The code then, parses the HTML or XML page, finds the data and extracts it. To extract data using web scraping with python, you need to follow these basic steps: Find the URL that you want to scrape. Inspecting the Page. Find the data you want to extract. Write the code. Run the code and extract the data.'}\n",
      "{'title': 'Web Scraping & NLP in Python | DataCamp', 'link': 'https://www.datacamp.com/tutorial/web-scraping-python-nlp', 'snippet': \"We won't give you the novels: you'll learn to scrape them from the website Project Gutenberg (which basically contains a large corpus of books) using the Python package requests and how to extract the novels from this web data using BeautifulSoup.Then you'll dive in to analyzing the novels using the Natural Language ToolKit (nltk).In the process, you'll learn about important aspects of Natural ...\"}\n",
      "{'title': 'How to Web Scrape with Python in 4 Minutes | by Julia Kho | Towards ...', 'link': 'https://towardsdatascience.com/how-to-web-scrape-with-python-in-4-minutes-bc49186a8460', 'snippet': 'It is important to understand the basics of HTML in order to successfully web scrape. On the website, right click and click on \"Inspect\". This allows you to see the raw code behind the site. Once you\\'ve clicked on \"Inspect\", you should see this console pop up. Console.'}\n",
      "{'title': 'Web Scraping With Python - Serpdog', 'link': 'https://serpdog.io/blog/web-scraping-with-python/', 'snippet': \"Web Scraping is the process of extracting a specific set of information from websites in the form of text, videos, images, and links. In today's world, web scraping is an important skill to learn, as it can be used for a variety of purposes, such as lead generation, price monitoring, SERP monitoring, etc. Web Scraping With Python — A ...\"}\n",
      "{'title': 'How to scrape data from arbitrary number of row listings using python ...', 'link': 'https://stackoverflow.com/questions/78639769/how-to-scrape-data-from-arbitrary-number-of-row-listings-using-python-selenium', 'snippet': \"This is super useful because you can try different selectors/different functions much quicker than re-running your script every time. (And this applies to any Python project, especially when working with something unfamiliar like web source code where you'll surely need to try different approaches) -\"}\n",
      "{'title': 'Web Scraping Financial Data Using Python - Nanonets', 'link': 'https://nanonets.com/blog/web-scraping-financial-data/', 'snippet': 'The web scraping process. The web scraping process follows a set of common principles across all tools and use cases. These principles stay the same for this entire web scraping process: Identify target URLs: Users need to manually select the URLs of websites that they want to extract data from and keep them ready to input into the web scraping ...'}\n",
      "{'title': 'Web Scraping Target.com Using Python - ScrapeHero', 'link': 'https://www.scrapehero.com/web-scraping-target-com/', 'snippet': 'Python is excellent for web scraping Target.com and storing the results in JSON format. Use Python requests to get the HTML code and the packages BeautifulSoup, json, and re to extract the content. The code in this tutorial will work until Target.com changes the JSON structure or URLs. If that happens, you need to update the code.'}\n",
      "{'title': 'Scraping Browser API - Automated Browser for Scraping - Bright Data', 'link': 'https://brightdata.com/products/scraping-browser', 'snippet': 'The term \"headless browser\" refers to a web browser without a graphical user interface. When used with a proxy, headless browsers can be used to scrape data, but they are easily detected by bot-protection software, making large-scale data scraping difficult. GUI browsers, like Scraping Browser (aka \"headfull\"), use a graphical user ...'}\n",
      "{'title': 'Mimic Human Behavior in Python Web Scraping - LinkedIn', 'link': 'https://www.linkedin.com/advice/3/how-can-you-mimic-human-browsing-behavior-python-rftde', 'snippet': 'In web development, Python is a powerful tool for web scraping, which is the process of extracting data from websites. However, simple scraping scripts can often be detected and blocked by websites.'}\n",
      "{'title': 'How to install Python on Windows, Linux, and macOS - XDA Developers', 'link': 'https://www.xda-developers.com/how-to-install-python/', 'snippet': \"Python 2.7 is often used for legacy projects, but if you're writing your own software, then you should install Python 3.x. To install on Windows or Mac, do the following:\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from langchain.loaders import BaseLoader\n",
    "from langchain_core.document_loaders.base import BaseBlobParser, BaseLoader\n",
    "\n",
    "class CustomLoader(BaseLoader):\n",
    "    def __init__(self, engine, query):\n",
    "        self.engine = engine\n",
    "        self.query = query\n",
    "\n",
    "    def load(self):\n",
    "        if self.engine == \"google\":\n",
    "            return GoogleSearchLoader(self.query).load()\n",
    "        elif self.engine == \"duckduckgo\":\n",
    "            return DuckDuckGoSearchLoader(self.query).load()\n",
    "        # elif self.engine == \"bing\":\n",
    "        #     return BingSearchLoader(self.query).load()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported search engine\")\n",
    "\n",
    "    def parse(self, html):\n",
    "        if self.engine == \"google\":\n",
    "            return GoogleSearchLoader(self.query).parse(html)\n",
    "        elif self.engine == \"duckduckgo\":\n",
    "            return DuckDuckGoSearchLoader(self.query).parse(html)\n",
    "        # elif self.engine == \"bing\":\n",
    "        #     return BingSearchLoader(self.query).parse(html)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported search engine\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = CustomLoader(engine=\"duckduckgo\", query=\"Python web scraping\")\n",
    "    print(f\"loader: {loader}\")\n",
    "    print(f\"type(loader): {type(loader)}\\n\")\n",
    "\n",
    "    html = loader.load()\n",
    "    #print(f\"\\nhtml: {html}\\n\")\n",
    "    #print() \n",
    "\n",
    "    parsed_results = loader.parse(html)\n",
    "    print(f\"length of parsed_results: {len(loader.parse(html))}\\n\")\n",
    "    print(f\"parsed_results: {loader.parse(html)}\\n\")\n",
    "\n",
    "    for result in parsed_results:\n",
    "        print(result)\n",
    "\n",
    "\n",
    "# This example demonstrates how to perform searches on Google, DuckDuckGo, and Bing using web scraping techniques, parse the results, and integrate them with LangChain for a custom loader. Make sure to handle the search engine's terms of service and usage policies regarding web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                title  \\\n",
      "0   Scrapy & Splash Python - Web Scraping with Python   \n",
      "1   Web Scraping With Python - Real-World Practice...   \n",
      "2   Beautiful Soup: Build a Web Scraper With Pytho...   \n",
      "3   A Practical Introduction to Web Scraping in Py...   \n",
      "4   Web Scraping Python Tutorial - How to Scrape D...   \n",
      "5   Python Web Scraping Using Beautiful Soup: A St...   \n",
      "6        Python Web Scraping Tutorial - GeeksforGeeks   \n",
      "7         Python Web Scraping Tutorials - Real Python   \n",
      "8   Web Scraping using Python (and Beautiful Soup)...   \n",
      "9   Python Web Scraping: Full Tutorial With Exampl...   \n",
      "10  Python Web Scraping Tutorial - How to Scrape D...   \n",
      "11  Python Web Scraping Tutorial: Step-By-Step [20...   \n",
      "12  A guide to web scraping in Python using Beauti...   \n",
      "13  Python Tutorial: Web Scraping with Scrapy (8 C...   \n",
      "14  Implementing Web Scraping in Python with Beaut...   \n",
      "15   Web Scraping with Python (A Comprehensive Guide)   \n",
      "16  Web scraping with Python: A quick guide - Educ...   \n",
      "17  Web Scraping With Python Libraries | LearnPyth...   \n",
      "18  Web Scraping Using Python: Step-by-Step Guide ...   \n",
      "19  Web Scraping With Python - 2024 Full Guide - B...   \n",
      "20           Web Scraping with Python - ScrapFly Blog   \n",
      "21  Web Scraping With Python - Full Guide to Pytho...   \n",
      "22            Web Scraping & NLP in Python | DataCamp   \n",
      "23  How to Web Scrape with Python in 4 Minutes | b...   \n",
      "24                 Web Scraping With Python - Serpdog   \n",
      "25  How to scrape data from arbitrary number of ro...   \n",
      "26  Web Scraping Financial Data Using Python - Nan...   \n",
      "27  Web Scraping Target.com Using Python - ScrapeHero   \n",
      "28  Scraping Browser API - Automated Browser for S...   \n",
      "29  Mimic Human Behavior in Python Web Scraping - ...   \n",
      "30  How to install Python on Windows, Linux, and m...   \n",
      "\n",
      "                                                 link  \\\n",
      "0   https://duckduckgo.com/y.js?ad_domain=udemy.co...   \n",
      "1   https://duckduckgo.com/y.js?ad_domain=codecade...   \n",
      "2   https://realpython.com/beautiful-soup-web-scra...   \n",
      "3   https://realpython.com/python-web-scraping-pra...   \n",
      "4   https://www.freecodecamp.org/news/web-scraping...   \n",
      "5   https://www.askpython.com/resources/python-web...   \n",
      "6   https://www.geeksforgeeks.org/python-web-scrap...   \n",
      "7      https://realpython.com/tutorials/web-scraping/   \n",
      "8   https://www.datacamp.com/tutorial/web-scraping...   \n",
      "9   https://www.scrapingbee.com/blog/web-scraping-...   \n",
      "10  https://www.freecodecamp.org/news/how-to-scrap...   \n",
      "11        https://oxylabs.io/blog/python-web-scraping   \n",
      "12  https://opensource.com/article/21/9/web-scrapi...   \n",
      "13  https://www.dataquest.io/blog/web-scraping-wit...   \n",
      "14  https://www.geeksforgeeks.org/implementing-web...   \n",
      "15  https://www.scrapingdog.com/blog/web-scraping-...   \n",
      "16  https://www.educative.io/blog/python-web-scrap...   \n",
      "17  https://learnpython.com/blog/python-web-scraping/   \n",
      "18    https://geekflare.com/web-scraping-with-python/   \n",
      "19  https://brightdata.com/blog/how-tos/web-scrapi...   \n",
      "20  https://scrapfly.io/blog/web-scraping-with-pyt...   \n",
      "21  https://www.edureka.co/blog/web-scraping-with-...   \n",
      "22  https://www.datacamp.com/tutorial/web-scraping...   \n",
      "23  https://towardsdatascience.com/how-to-web-scra...   \n",
      "24  https://serpdog.io/blog/web-scraping-with-python/   \n",
      "25  https://stackoverflow.com/questions/78639769/h...   \n",
      "26  https://nanonets.com/blog/web-scraping-financi...   \n",
      "27  https://www.scrapehero.com/web-scraping-target...   \n",
      "28   https://brightdata.com/products/scraping-browser   \n",
      "29  https://www.linkedin.com/advice/3/how-can-you-...   \n",
      "30  https://www.xda-developers.com/how-to-install-...   \n",
      "\n",
      "                                              snippet  \n",
      "0   Join Millions of Learners from Around the Worl...  \n",
      "1   Take your skills to a new level and join milli...  \n",
      "2   Learn how to use requests and Beautiful Soup l...  \n",
      "3   Learn how to collect and parse data from websi...  \n",
      "4   Learn how to use Python and BeautifulSoup to s...  \n",
      "5   Do this to scrape a website with BeautifulSoup...  \n",
      "6   Learn how to extract data from websites using ...  \n",
      "7   Learn how to use Python to download and select...  \n",
      "8   Learn how to extract, manipulate and visualize...  \n",
      "9   Learn how to scrape web data with Python using...  \n",
      "10  Learn how to scrape data from any website with...  \n",
      "11  Learn how to use Python libraries like request...  \n",
      "12  Learn how to use the requests and Beautiful So...  \n",
      "13  In this Python tutorial, we'll go over web scr...  \n",
      "14  Steps involved in web scraping: Send an HTTP r...  \n",
      "15  Learn how to scrape data from any website usin...  \n",
      "16  Overview: Web scraping with Python. Build a we...  \n",
      "17  Web scraping is the process of extracting info...  \n",
      "18  Web scraping using Python is a very popular ch...  \n",
      "19  Web scraping is about extracting data from the...  \n",
      "20  In this python web scraping tutorial, we've co...  \n",
      "21  The code then, parses the HTML or XML page, fi...  \n",
      "22  We won't give you the novels: you'll learn to ...  \n",
      "23  It is important to understand the basics of HT...  \n",
      "24  Web Scraping is the process of extracting a sp...  \n",
      "25  This is super useful because you can try diffe...  \n",
      "26  The web scraping process. The web scraping pro...  \n",
      "27  Python is excellent for web scraping Target.co...  \n",
      "28  The term \"headless browser\" refers to a web br...  \n",
      "29  In web development, Python is a powerful tool ...  \n",
      "30  Python 2.7 is often used for legacy projects, ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# Assuming parsed_results is a list of dictionaries\n",
    "df = pd.DataFrame(parsed_results)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
