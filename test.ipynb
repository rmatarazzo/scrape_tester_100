{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content saved to google_search_results_20240624_153001.html\n",
      "Extracted 102 items.\n",
      "Parsed search results saved to parsed_search_results_20240624_153002.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    \"\"\"Generate a filename with a timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "class GoogleSearchLoader:\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def load_and_scroll(self):\n",
    "        \"\"\"Load search results and scroll until at least 100 items or no new items are found.\"\"\"\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(f\"https://www.google.com/search?q={quote_plus(self.query)}\")\n",
    "\n",
    "        num_results = 0\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while num_results < 100:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(20)  # Increased delay to allow more content to load\n",
    "            \n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, 'a#pnnext')\n",
    "                    next_button.click()\n",
    "                    time.sleep(20)  # Wait for the next page to load\n",
    "                    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        more_results_button = driver.find_element(By.XPATH, '//a[contains(., \"More results\")]')\n",
    "                        more_results_button.click()\n",
    "                        time.sleep(20)  # Wait for more results to load\n",
    "                        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    except Exception:\n",
    "                        print(\"No more pages or unable to load more results.\")\n",
    "                        break  # No new items or next/more results button was not found\n",
    "            else:\n",
    "                last_height = new_height\n",
    "            \n",
    "            search_items = driver.find_elements(By.CSS_SELECTOR, 'div.g')\n",
    "            num_results = len(search_items)\n",
    "        \n",
    "        html_content = driver.page_source\n",
    "        driver.quit()\n",
    "        return html_content\n",
    "\n",
    "    def parse_items(self, html_content):\n",
    "        \"\"\"Parse search items from HTML content using BeautifulSoup.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        results = []\n",
    "        search_items = soup.select('div.g')  # Adjust the selector as needed\n",
    "        for item in search_items:\n",
    "            title_div = item.select_one('h3')\n",
    "            title = title_div.text if title_div else \"No title\"\n",
    "            link_tag = item.select_one('a')\n",
    "            link = link_tag['href'] if link_tag else None\n",
    "            snippet_div = item.select_one('div.kb0PBd')\n",
    "            snippet = snippet_div.text if snippet_div else \"No snippet available\"\n",
    "            if link:\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "        return results\n",
    "\n",
    "    def save_html(self, html_content, base_filename=\"google_search_results.html\"):\n",
    "        \"\"\"Save the HTML content to a timestamped file.\"\"\"\n",
    "        timestamped_file = timestamped_filename(base_filename)\n",
    "        with open(timestamped_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "        return timestamped_file\n",
    "\n",
    "    def save_results_to_json(self, results, base_filename=\"parsed_search_results.json\"):\n",
    "        \"\"\"Save the parsed results to a timestamped JSON file.\"\"\"\n",
    "        json_filename = timestamped_filename(base_filename)\n",
    "        with open(json_filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "        return json_filename\n",
    "\n",
    "def main():\n",
    "    # Create a Tkinter root window and hide it\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Prompt the user for the search query using Tkinter\n",
    "    query = simpledialog.askstring(\"Input\", \"Please enter the search query topic:\")\n",
    "\n",
    "    # Check if the user provided a query\n",
    "    if query:\n",
    "        loader = GoogleSearchLoader(query=query)\n",
    "        \n",
    "        # Load and scroll through the search results, then save the HTML content\n",
    "        html_content = loader.load_and_scroll()\n",
    "        html_filename = loader.save_html(html_content)\n",
    "        print(f\"HTML content saved to {html_filename}\")\n",
    "\n",
    "        # Parse the HTML content to extract search results\n",
    "        search_results = loader.parse_items(html_content)\n",
    "        print(f\"Extracted {len(search_results)} items.\")\n",
    "\n",
    "        # Save the parsed search results to a JSON file\n",
    "        json_filename = loader.save_results_to_json(search_results)\n",
    "        print(f\"Parsed search results saved to {json_filename}\")\n",
    "    else:\n",
    "        print(\"No query provided. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "def get_page_title(link):\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(link)\n",
    "        title = driver.title\n",
    "        driver.quit()\n",
    "        return title, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def test_link(link_info):\n",
    "    link = link_info['link']\n",
    "    title, error = get_page_title(link)\n",
    "    if error:\n",
    "        return {'link': link, 'status': 'error', 'error': error}\n",
    "    else:\n",
    "        return {'link': link, 'status': 'success', 'title': title}\n",
    "\n",
    "def main():\n",
    "    with open('parsed_search_results_20240624_153002.json', 'r', encoding='utf-8') as f:\n",
    "        search_results = json.load(f)\n",
    "\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(test_link, item): item for item in search_results}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "\n",
    "    output_filename = timestamped_filename('links_scrape_test.json')\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    \"\"\"Generate a filename with a timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "class GoogleSearchLoader:\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def load_and_scroll(self):\n",
    "        \"\"\"Load search results and scroll until at least 100 items or no new items are found.\"\"\"\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(f\"https://www.google.com/search?q={quote_plus(self.query)}\")\n",
    "\n",
    "        num_results = 0\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while num_results < 100:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(20)  # Increased delay to allow more content to load\n",
    "\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, 'a#pnnext')\n",
    "                    next_button.click()\n",
    "                    time.sleep(20)  # Wait for the next page to load\n",
    "                except Exception:\n",
    "                    break  # No more pages to load\n",
    "            last_height = new_height\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            items = soup.select('.tF2Cxc')\n",
    "            num_results = len(items)\n",
    "\n",
    "        html_content = driver.page_source\n",
    "        driver.quit()\n",
    "        return html_content\n",
    "\n",
    "    def parse_items(self, html_content):\n",
    "        \"\"\"Parse items from the HTML content.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        results = []\n",
    "\n",
    "        for item in soup.select('.tF2Cxc'):\n",
    "            title_element = item.select_one('.DKV0Md')\n",
    "            link_element = item.select_one('.yuRUbf > a')\n",
    "            snippet_element = item.select_one('.IsZvec')\n",
    "\n",
    "            title = title_element.text if title_element else None\n",
    "            link = link_element['href'] if link_element else None\n",
    "            snippet = snippet_element.text if snippet_element else None\n",
    "\n",
    "            if link:\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "        return results\n",
    "\n",
    "    def save_html(self, html_content, base_filename=\"google_search_results.html\"):\n",
    "        \"\"\"Save the HTML content to a timestamped file.\"\"\"\n",
    "        timestamped_file = timestamped_filename(base_filename)\n",
    "        with open(timestamped_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "        return timestamped_file\n",
    "\n",
    "    def save_results_to_json(self, results, base_filename=\"parsed_search_results.json\"):\n",
    "        \"\"\"Save the parsed results to a timestamped JSON file.\"\"\"\n",
    "        json_filename = timestamped_filename(base_filename)\n",
    "        with open(json_filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "        return json_filename\n",
    "\n",
    "def get_page_title(link):\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(link)\n",
    "        title = driver.title\n",
    "        driver.quit()\n",
    "        return title, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def test_link(link_info):\n",
    "    link = link_info['link']\n",
    "    title, error = get_page_title(link)\n",
    "    if error:\n",
    "        return {'link': link, 'status': 'error', 'error': error}\n",
    "    else:\n",
    "        return {'link': link, 'status': 'success', 'title': title}\n",
    "\n",
    "def test_links_concurrently(search_results):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(test_link, item): item for item in search_results}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Create a Tkinter root window and hide it\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Prompt the user for the search query using Tkinter\n",
    "    query = simpledialog.askstring(\"Input\", \"Please enter the search query topic:\")\n",
    "\n",
    "    # Check if the user provided a query\n",
    "    if query:\n",
    "        loader = GoogleSearchLoader(query=query)\n",
    "\n",
    "        # Load and scroll through the search results, then save the HTML content\n",
    "        html_content = loader.load_and_scroll()\n",
    "        html_filename = loader.save_html(html_content)\n",
    "        print(f\"HTML content saved to {html_filename}\")\n",
    "\n",
    "        # Parse the HTML content to extract search results\n",
    "        search_results = loader.parse_items(html_content)\n",
    "        print(f\"Extracted {len(search_results)} items.\")\n",
    "\n",
    "        # Save the parsed search results to a JSON file\n",
    "        json_filename = loader.save_results_to_json(search_results)\n",
    "        print(f\"Parsed search results saved to {json_filename}\")\n",
    "\n",
    "        # Load the parsed search results from the JSON file\n",
    "        with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "            search_results = json.load(f)\n",
    "\n",
    "        # Test the links concurrently and save the results\n",
    "        test_results = test_links_concurrently(search_results)\n",
    "        output_filename = timestamped_filename('links_scrape_test.json')\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Test results saved to {output_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No query provided. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content saved to google_search_results_20240624_171355.html\n",
      "Extracted 101 items.\n",
      "Parsed search results saved to parsed_search_results_20240624_171356.json\n",
      "Test results saved to links_scrape_test_20240624_171835.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    \"\"\"Generate a filename with a timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "class GoogleSearchLoader:\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def load_and_scroll(self):\n",
    "        \"\"\"Load search results and scroll until at least 100 items or no new items are found.\"\"\"\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(f\"https://www.google.com/search?q={quote_plus(self.query)}\")\n",
    "\n",
    "        num_results = 0\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while num_results < 100:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(20)  # Increased delay to allow more content to load\n",
    "            \n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, 'a#pnnext')\n",
    "                    next_button.click()\n",
    "                    time.sleep(20)  # Wait for the next page to load\n",
    "                    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        more_results_button = driver.find_element(By.XPATH, '//a[contains(., \"More results\")]')\n",
    "                        more_results_button.click()\n",
    "                        time.sleep(20)  # Wait for more results to load\n",
    "                        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    except Exception:\n",
    "                        print(\"No more pages or unable to load more results.\")\n",
    "                        break  # No new items or next/more results button was not found\n",
    "            else:\n",
    "                last_height = new_height\n",
    "            \n",
    "            search_items = driver.find_elements(By.CSS_SELECTOR, 'div.g')\n",
    "            num_results = len(search_items)\n",
    "        \n",
    "        html_content = driver.page_source\n",
    "        driver.quit()\n",
    "        return html_content\n",
    "\n",
    "    def parse_items(self, html_content):\n",
    "        \"\"\"Parse search items from HTML content using BeautifulSoup.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        results = []\n",
    "        search_items = soup.select('div.g')  # Adjust the selector as needed\n",
    "        for item in search_items:\n",
    "            title_div = item.select_one('h3')\n",
    "            title = title_div.text if title_div else \"No title\"\n",
    "            link_tag = item.select_one('a')\n",
    "            link = link_tag['href'] if link_tag else None\n",
    "            snippet_div = item.select_one('div.kb0PBd')\n",
    "            snippet = snippet_div.text if snippet_div else \"No snippet available\"\n",
    "            if link:\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "        return results\n",
    "\n",
    "    def save_html(self, html_content, base_filename=\"google_search_results.html\"):\n",
    "        \"\"\"Save the HTML content to a timestamped file.\"\"\"\n",
    "        timestamped_file = timestamped_filename(base_filename)\n",
    "        with open(timestamped_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "        return timestamped_file\n",
    "\n",
    "    def save_results_to_json(self, results, base_filename=\"parsed_search_results.json\"):\n",
    "        \"\"\"Save the parsed results to a timestamped JSON file.\"\"\"\n",
    "        json_filename = timestamped_filename(base_filename)\n",
    "        with open(json_filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "        return json_filename\n",
    "\n",
    "def get_page_title(link):\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(link)\n",
    "        title = driver.title\n",
    "        driver.quit()\n",
    "        return title, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def test_link(link_info):\n",
    "    link = link_info['link']\n",
    "    title, error = get_page_title(link)\n",
    "    if error:\n",
    "        return {'link': link, 'status': 'error', 'error': error}\n",
    "    else:\n",
    "        return {'link': link, 'status': 'success', 'title': title}\n",
    "\n",
    "def test_links_concurrently(search_results):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(test_link, item): item for item in search_results}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Create a Tkinter root window and hide it\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Prompt the user for the search query using Tkinter\n",
    "    query = simpledialog.askstring(\"Input\", \"Please enter the search query topic:\")\n",
    "\n",
    "    # Check if the user provided a query\n",
    "    if query:\n",
    "        loader = GoogleSearchLoader(query=query)\n",
    "        \n",
    "        # Load and scroll through the search results, then save the HTML content\n",
    "        html_content = loader.load_and_scroll()\n",
    "        html_filename = loader.save_html(html_content)\n",
    "        print(f\"HTML content saved to {html_filename}\")\n",
    "\n",
    "        # Parse the HTML content to extract search results\n",
    "        search_results = loader.parse_items(html_content)\n",
    "        print(f\"Extracted {len(search_results)} items.\")\n",
    "\n",
    "        # Save the parsed search results to a JSON file\n",
    "        json_filename = loader.save_results_to_json(search_results)\n",
    "        print(f\"Parsed search results saved to {json_filename}\")\n",
    "\n",
    "        # Load the parsed search results from the JSON file\n",
    "        with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "            search_results = json.load(f)\n",
    "\n",
    "        # Test the links concurrently and save the results\n",
    "        test_results = test_links_concurrently(search_results)\n",
    "        output_filename = timestamped_filename('links_scrape_test.json')\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Test results saved to {output_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No query provided. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape_tester_100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
