{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content saved to google_search_results_20240624_153001.html\n",
      "Extracted 102 items.\n",
      "Parsed search results saved to parsed_search_results_20240624_153002.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    \"\"\"Generate a filename with a timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "class GoogleSearchLoader:\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def load_and_scroll(self):\n",
    "        \"\"\"Load search results and scroll until at least 100 items or no new items are found.\"\"\"\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(f\"https://www.google.com/search?q={quote_plus(self.query)}\")\n",
    "\n",
    "        num_results = 0\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while num_results < 100:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(20)  # Increased delay to allow more content to load\n",
    "            \n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, 'a#pnnext')\n",
    "                    next_button.click()\n",
    "                    time.sleep(20)  # Wait for the next page to load\n",
    "                    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        more_results_button = driver.find_element(By.XPATH, '//a[contains(., \"More results\")]')\n",
    "                        more_results_button.click()\n",
    "                        time.sleep(20)  # Wait for more results to load\n",
    "                        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    except Exception:\n",
    "                        print(\"No more pages or unable to load more results.\")\n",
    "                        break  # No new items or next/more results button was not found\n",
    "            else:\n",
    "                last_height = new_height\n",
    "            \n",
    "            search_items = driver.find_elements(By.CSS_SELECTOR, 'div.g')\n",
    "            num_results = len(search_items)\n",
    "        \n",
    "        html_content = driver.page_source\n",
    "        driver.quit()\n",
    "        return html_content\n",
    "\n",
    "    def parse_items(self, html_content):\n",
    "        \"\"\"Parse search items from HTML content using BeautifulSoup.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        results = []\n",
    "        search_items = soup.select('div.g')  # Adjust the selector as needed\n",
    "        for item in search_items:\n",
    "            title_div = item.select_one('h3')\n",
    "            title = title_div.text if title_div else \"No title\"\n",
    "            link_tag = item.select_one('a')\n",
    "            link = link_tag['href'] if link_tag else None\n",
    "            snippet_div = item.select_one('div.kb0PBd')\n",
    "            snippet = snippet_div.text if snippet_div else \"No snippet available\"\n",
    "            if link:\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "        return results\n",
    "\n",
    "    def save_html(self, html_content, base_filename=\"google_search_results.html\"):\n",
    "        \"\"\"Save the HTML content to a timestamped file.\"\"\"\n",
    "        timestamped_file = timestamped_filename(base_filename)\n",
    "        with open(timestamped_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "        return timestamped_file\n",
    "\n",
    "    def save_results_to_json(self, results, base_filename=\"parsed_search_results.json\"):\n",
    "        \"\"\"Save the parsed results to a timestamped JSON file.\"\"\"\n",
    "        json_filename = timestamped_filename(base_filename)\n",
    "        with open(json_filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "        return json_filename\n",
    "\n",
    "def main():\n",
    "    # Create a Tkinter root window and hide it\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Prompt the user for the search query using Tkinter\n",
    "    query = simpledialog.askstring(\"Input\", \"Please enter the search query topic:\")\n",
    "\n",
    "    # Check if the user provided a query\n",
    "    if query:\n",
    "        loader = GoogleSearchLoader(query=query)\n",
    "        \n",
    "        # Load and scroll through the search results, then save the HTML content\n",
    "        html_content = loader.load_and_scroll()\n",
    "        html_filename = loader.save_html(html_content)\n",
    "        print(f\"HTML content saved to {html_filename}\")\n",
    "\n",
    "        # Parse the HTML content to extract search results\n",
    "        search_results = loader.parse_items(html_content)\n",
    "        print(f\"Extracted {len(search_results)} items.\")\n",
    "\n",
    "        # Save the parsed search results to a JSON file\n",
    "        json_filename = loader.save_results_to_json(search_results)\n",
    "        print(f\"Parsed search results saved to {json_filename}\")\n",
    "    else:\n",
    "        print(\"No query provided. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "def get_page_title(link):\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(link)\n",
    "        title = driver.title\n",
    "        driver.quit()\n",
    "        return title, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def test_link(link_info):\n",
    "    link = link_info['link']\n",
    "    title, error = get_page_title(link)\n",
    "    if error:\n",
    "        return {'link': link, 'status': 'error', 'error': error}\n",
    "    else:\n",
    "        return {'link': link, 'status': 'success', 'title': title}\n",
    "\n",
    "def main():\n",
    "    with open('parsed_search_results_20240624_153002.json', 'r', encoding='utf-8') as f:\n",
    "        search_results = json.load(f)\n",
    "\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(test_link, item): item for item in search_results}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "\n",
    "    output_filename = timestamped_filename('links_scrape_test.json')\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    \"\"\"Generate a filename with a timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "class GoogleSearchLoader:\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def load_and_scroll(self):\n",
    "        \"\"\"Load search results and scroll until at least 100 items or no new items are found.\"\"\"\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(f\"https://www.google.com/search?q={quote_plus(self.query)}\")\n",
    "\n",
    "        num_results = 0\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while num_results < 100:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(20)  # Increased delay to allow more content to load\n",
    "\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, 'a#pnnext')\n",
    "                    next_button.click()\n",
    "                    time.sleep(20)  # Wait for the next page to load\n",
    "                except Exception:\n",
    "                    break  # No more pages to load\n",
    "            last_height = new_height\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            items = soup.select('.tF2Cxc')\n",
    "            num_results = len(items)\n",
    "\n",
    "        html_content = driver.page_source\n",
    "        driver.quit()\n",
    "        return html_content\n",
    "\n",
    "    def parse_items(self, html_content):\n",
    "        \"\"\"Parse items from the HTML content.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        results = []\n",
    "\n",
    "        for item in soup.select('.tF2Cxc'):\n",
    "            title_element = item.select_one('.DKV0Md')\n",
    "            link_element = item.select_one('.yuRUbf > a')\n",
    "            snippet_element = item.select_one('.IsZvec')\n",
    "\n",
    "            title = title_element.text if title_element else None\n",
    "            link = link_element['href'] if link_element else None\n",
    "            snippet = snippet_element.text if snippet_element else None\n",
    "\n",
    "            if link:\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "        return results\n",
    "\n",
    "    def save_html(self, html_content, base_filename=\"google_search_results.html\"):\n",
    "        \"\"\"Save the HTML content to a timestamped file.\"\"\"\n",
    "        timestamped_file = timestamped_filename(base_filename)\n",
    "        with open(timestamped_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "        return timestamped_file\n",
    "\n",
    "    def save_results_to_json(self, results, base_filename=\"parsed_search_results.json\"):\n",
    "        \"\"\"Save the parsed results to a timestamped JSON file.\"\"\"\n",
    "        json_filename = timestamped_filename(base_filename)\n",
    "        with open(json_filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "        return json_filename\n",
    "\n",
    "def get_page_title(link):\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(link)\n",
    "        title = driver.title\n",
    "        driver.quit()\n",
    "        return title, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def test_link(link_info):\n",
    "    link = link_info['link']\n",
    "    title, error = get_page_title(link)\n",
    "    if error:\n",
    "        return {'link': link, 'status': 'error', 'error': error}\n",
    "    else:\n",
    "        return {'link': link, 'status': 'success', 'title': title}\n",
    "\n",
    "def test_links_concurrently(search_results):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(test_link, item): item for item in search_results}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Create a Tkinter root window and hide it\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Prompt the user for the search query using Tkinter\n",
    "    query = simpledialog.askstring(\"Input\", \"Please enter the search query topic:\")\n",
    "\n",
    "    # Check if the user provided a query\n",
    "    if query:\n",
    "        loader = GoogleSearchLoader(query=query)\n",
    "\n",
    "        # Load and scroll through the search results, then save the HTML content\n",
    "        html_content = loader.load_and_scroll()\n",
    "        html_filename = loader.save_html(html_content)\n",
    "        print(f\"HTML content saved to {html_filename}\")\n",
    "\n",
    "        # Parse the HTML content to extract search results\n",
    "        search_results = loader.parse_items(html_content)\n",
    "        print(f\"Extracted {len(search_results)} items.\")\n",
    "\n",
    "        # Save the parsed search results to a JSON file\n",
    "        json_filename = loader.save_results_to_json(search_results)\n",
    "        print(f\"Parsed search results saved to {json_filename}\")\n",
    "\n",
    "        # Load the parsed search results from the JSON file\n",
    "        with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "            search_results = json.load(f)\n",
    "\n",
    "        # Test the links concurrently and save the results\n",
    "        test_results = test_links_concurrently(search_results)\n",
    "        output_filename = timestamped_filename('links_scrape_test.json')\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Test results saved to {output_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No query provided. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content saved to google_search_results_20240624_171355.html\n",
      "Extracted 101 items.\n",
      "Parsed search results saved to parsed_search_results_20240624_171356.json\n",
      "Test results saved to links_scrape_test_20240624_171835.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    \"\"\"Generate a filename with a timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "class GoogleSearchLoader:\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def load_and_scroll(self):\n",
    "        \"\"\"Load search results and scroll until at least 100 items or no new items are found.\"\"\"\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(f\"https://www.google.com/search?q={quote_plus(self.query)}\")\n",
    "\n",
    "        num_results = 0\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while num_results < 100:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(20)  # Increased delay to allow more content to load\n",
    "            \n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, 'a#pnnext')\n",
    "                    next_button.click()\n",
    "                    time.sleep(20)  # Wait for the next page to load\n",
    "                    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        more_results_button = driver.find_element(By.XPATH, '//a[contains(., \"More results\")]')\n",
    "                        more_results_button.click()\n",
    "                        time.sleep(20)  # Wait for more results to load\n",
    "                        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    except Exception:\n",
    "                        print(\"No more pages or unable to load more results.\")\n",
    "                        break  # No new items or next/more results button was not found\n",
    "            else:\n",
    "                last_height = new_height\n",
    "            \n",
    "            search_items = driver.find_elements(By.CSS_SELECTOR, 'div.g')\n",
    "            num_results = len(search_items)\n",
    "        \n",
    "        html_content = driver.page_source\n",
    "        driver.quit()\n",
    "        return html_content\n",
    "\n",
    "    def parse_items(self, html_content):\n",
    "        \"\"\"Parse search items from HTML content using BeautifulSoup.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        results = []\n",
    "        search_items = soup.select('div.g')  # Adjust the selector as needed\n",
    "        for item in search_items:\n",
    "            title_div = item.select_one('h3')\n",
    "            title = title_div.text if title_div else \"No title\"\n",
    "            link_tag = item.select_one('a')\n",
    "            link = link_tag['href'] if link_tag else None\n",
    "            snippet_div = item.select_one('div.kb0PBd')\n",
    "            snippet = snippet_div.text if snippet_div else \"No snippet available\"\n",
    "            if link:\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "        return results\n",
    "\n",
    "    def save_html(self, html_content, base_filename=\"google_search_results.html\"):\n",
    "        \"\"\"Save the HTML content to a timestamped file.\"\"\"\n",
    "        timestamped_file = timestamped_filename(base_filename)\n",
    "        with open(timestamped_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "        return timestamped_file\n",
    "\n",
    "    def save_results_to_json(self, results, base_filename=\"parsed_search_results.json\"):\n",
    "        \"\"\"Save the parsed results to a timestamped JSON file.\"\"\"\n",
    "        json_filename = timestamped_filename(base_filename)\n",
    "        with open(json_filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "        return json_filename\n",
    "\n",
    "def get_page_title(link):\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(link)\n",
    "        title = driver.title\n",
    "        driver.quit()\n",
    "        return title, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def test_link(link_info):\n",
    "    link = link_info['link']\n",
    "    title, error = get_page_title(link)\n",
    "    if error:\n",
    "        return {'link': link, 'status': 'error', 'error': error}\n",
    "    else:\n",
    "        return {'link': link, 'status': 'success', 'title': title}\n",
    "\n",
    "def test_links_concurrently(search_results):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(test_link, item): item for item in search_results}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Create a Tkinter root window and hide it\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Prompt the user for the search query using Tkinter\n",
    "    query = simpledialog.askstring(\"Input\", \"Please enter the search query topic:\")\n",
    "\n",
    "    # Check if the user provided a query\n",
    "    if query:\n",
    "        loader = GoogleSearchLoader(query=query)\n",
    "        \n",
    "        # Load and scroll through the search results, then save the HTML content\n",
    "        html_content = loader.load_and_scroll()\n",
    "        html_filename = loader.save_html(html_content)\n",
    "        print(f\"HTML content saved to {html_filename}\")\n",
    "\n",
    "        # Parse the HTML content to extract search results\n",
    "        search_results = loader.parse_items(html_content)\n",
    "        print(f\"Extracted {len(search_results)} items.\")\n",
    "\n",
    "        # Save the parsed search results to a JSON file\n",
    "        json_filename = loader.save_results_to_json(search_results)\n",
    "        print(f\"Parsed search results saved to {json_filename}\")\n",
    "\n",
    "        # Load the parsed search results from the JSON file\n",
    "        with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "            search_results = json.load(f)\n",
    "\n",
    "        # Test the links concurrently and save the results\n",
    "        test_results = test_links_concurrently(search_results)\n",
    "        output_filename = timestamped_filename('links_scrape_test.json')\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Test results saved to {output_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No query provided. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content saved to google_search_results_20240624_173846.html\n",
      "Extracted 102 items.\n",
      "Parsed search results saved to parsed_search_results_20240624_173847.json\n",
      "Test results saved to links_scrape_test_20240624_174440.json\n",
      "Report saved to links_scrape_report_20240624_174440.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    \"\"\"Generate a filename with a timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "class GoogleSearchLoader:\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def load_and_scroll(self):\n",
    "        \"\"\"Load search results and scroll until at least 100 items or no new items are found.\"\"\"\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(f\"https://www.google.com/search?q={quote_plus(self.query)}\")\n",
    "\n",
    "        num_results = 0\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while num_results < 100:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(20)  # Increased delay to allow more content to load\n",
    "            \n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, 'a#pnnext')\n",
    "                    next_button.click()\n",
    "                    time.sleep(20)  # Wait for the next page to load\n",
    "                    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        more_results_button = driver.find_element(By.XPATH, '//a[contains(., \"More results\")]')\n",
    "                        more_results_button.click()\n",
    "                        time.sleep(20)  # Wait for more results to load\n",
    "                        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    except Exception:\n",
    "                        print(\"No more pages or unable to load more results.\")\n",
    "                        break  # No new items or next/more results button was not found\n",
    "            else:\n",
    "                last_height = new_height\n",
    "            \n",
    "            search_items = driver.find_elements(By.CSS_SELECTOR, 'div.g')\n",
    "            num_results = len(search_items)\n",
    "        \n",
    "        html_content = driver.page_source\n",
    "        driver.quit()\n",
    "        return html_content\n",
    "\n",
    "    def parse_items(self, html_content):\n",
    "        \"\"\"Parse search items from HTML content using BeautifulSoup.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        results = []\n",
    "        search_items = soup.select('div.g')  # Adjust the selector as needed\n",
    "        for item in search_items:\n",
    "            title_div = item.select_one('h3')\n",
    "            title = title_div.text if title_div else \"No title\"\n",
    "            link_tag = item.select_one('a')\n",
    "            link = link_tag['href'] if link_tag else None\n",
    "            snippet_div = item.select_one('div.kb0PBd')\n",
    "            snippet = snippet_div.text if snippet_div else \"No snippet available\"\n",
    "            if link:\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "        return results\n",
    "\n",
    "    def save_html(self, html_content, base_filename=\"google_search_results.html\"):\n",
    "        \"\"\"Save the HTML content to a timestamped file.\"\"\"\n",
    "        timestamped_file = timestamped_filename(base_filename)\n",
    "        with open(timestamped_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "        return timestamped_file\n",
    "\n",
    "    def save_results_to_json(self, results, base_filename=\"parsed_search_results.json\"):\n",
    "        \"\"\"Save the parsed results to a timestamped JSON file.\"\"\"\n",
    "        json_filename = timestamped_filename(base_filename)\n",
    "        with open(json_filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "        return json_filename\n",
    "\n",
    "def get_page_title(link):\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(link)\n",
    "        title = driver.title\n",
    "        driver.quit()\n",
    "        return title, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def test_link(link_info):\n",
    "    link = link_info['link']\n",
    "    title, error = get_page_title(link)\n",
    "    if error:\n",
    "        return {'link': link, 'status': 'error', 'error': error}\n",
    "    else:\n",
    "        return {'link': link, 'status': 'success', 'title': title}\n",
    "\n",
    "def test_links_concurrently(search_results):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(test_link, item): item for item in search_results}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def generate_report(test_results):\n",
    "    success_count = sum(1 for result in test_results if result['status'] == 'success')\n",
    "    error_count = len(test_results) - success_count\n",
    "\n",
    "    report_lines = [\n",
    "        \"Link Scrape Test Report\",\n",
    "        f\"Total Links Tested: {len(test_results)}\",\n",
    "        f\"Total Successes: {success_count}\",\n",
    "        f\"Total Errors: {error_count}\",\n",
    "        \"\",\n",
    "        \"Details:\"\n",
    "    ]\n",
    "\n",
    "    for result in test_results:\n",
    "        if result['status'] == 'success':\n",
    "            report_lines.append(f\"SUCCESS: {result['link']} - Title: {result['title']}\")\n",
    "        else:\n",
    "            report_lines.append(f\"ERROR: {result['link']} - Error: {result['error']}\")\n",
    "\n",
    "    return \"\\n\".join(report_lines)\n",
    "\n",
    "def main():\n",
    "    # Create a Tkinter root window and hide it\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Prompt the user for the search query using Tkinter\n",
    "    query = simpledialog.askstring(\"Input\", \"Please enter the search query topic:\")\n",
    "\n",
    "    # Check if the user provided a query\n",
    "    if query:\n",
    "        loader = GoogleSearchLoader(query=query)\n",
    "        \n",
    "        # Load and scroll through the search results, then save the HTML content\n",
    "        html_content = loader.load_and_scroll()\n",
    "        html_filename = loader.save_html(html_content)\n",
    "        print(f\"HTML content saved to {html_filename}\")\n",
    "\n",
    "        # Parse the HTML content to extract search results\n",
    "        search_results = loader.parse_items(html_content)\n",
    "        print(f\"Extracted {len(search_results)} items.\")\n",
    "\n",
    "        # Save the parsed search results to a JSON file\n",
    "        json_filename = loader.save_results_to_json(search_results)\n",
    "        print(f\"Parsed search results saved to {json_filename}\")\n",
    "\n",
    "        # Load the parsed search results from the JSON file\n",
    "        with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "            search_results = json.load(f)\n",
    "\n",
    "        # Test the links concurrently and save the results\n",
    "        test_results = test_links_concurrently(search_results)\n",
    "        output_filename = timestamped_filename('links_scrape_test.json')\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Test results saved to {output_filename}\")\n",
    "\n",
    "        # Generate the report\n",
    "        report = generate_report(test_results)\n",
    "        report_filename = timestamped_filename('links_scrape_report.txt')\n",
    "        with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        print(f\"Report saved to {report_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No query provided. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def can_be_scraped(url, expected_element, element_attribute=None, attribute_value=None):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            if element_attribute and attribute_value:\n",
    "                elements = soup.find_all(expected_element, {element_attribute: attribute_value})\n",
    "            else:\n",
    "                elements = soup.find_all(expected_element)\n",
    "            return len(elements) > 0\n",
    "        else:\n",
    "            print(f\"Failed to retrieve webpage. Status code: {response.status_code}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: /finance.yahoo.com (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016433F68F50>: Failed to resolve 'https' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Webpage cannot be scraped.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def can_be_scraped(url, expected_element, element_attribute=None, attribute_value=None):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Check for the expected element\n",
    "            if element_attribute and attribute_value:\n",
    "                elements = soup.find_all(expected_element, {element_attribute: attribute_value})\n",
    "            else:\n",
    "                elements = soup.find_all(expected_element)\n",
    "            \n",
    "            if len(elements) > 0:\n",
    "                # Extract metadata if the scrape test passes\n",
    "                metadata = {\n",
    "                    'title': soup.find('title').get_text() if soup.find('title') else None,\n",
    "                    'description': None,\n",
    "                    'keywords': None\n",
    "                }\n",
    "\n",
    "                description_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "                if description_tag and 'content' in description_tag.attrs:\n",
    "                    metadata['description'] = description_tag['content']\n",
    "\n",
    "                keywords_tag = soup.find('meta', attrs={'name': 'keywords'})\n",
    "                if keywords_tag and 'content' in keywords_tag.attrs:\n",
    "                    metadata['keywords'] = keywords_tag['content']\n",
    "\n",
    "                return True, metadata\n",
    "            else:\n",
    "                return False, {}\n",
    "        else:\n",
    "            print(f\"Failed to retrieve webpage. Status code: {response.status_code}\")\n",
    "            return False, {}\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False, {}\n",
    "\n",
    "# Example usage\n",
    "url = 'https://https://finance.yahoo.com'\n",
    "expected_element = 'h1'\n",
    "element_attribute = 'class'\n",
    "attribute_value = 'header'\n",
    "\n",
    "can_scrape, metadata = can_be_scraped(url, expected_element, element_attribute, attribute_value)\n",
    "if can_scrape:\n",
    "    print(\"Webpage can be scraped. Metadata:\")\n",
    "    print(metadata)\n",
    "else:\n",
    "    print(\"Webpage cannot be scraped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages or unable to load more results.\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: disconnected: not connected to DevTools\n  (failed to check if window was closed: disconnected: not connected to DevTools)\n  (Session info: chrome=126.0.6478.127)\nStacktrace:\n\tGetHandleVerifier [0x0083C1C3+27395]\n\t(No symbol) [0x007D3DC4]\n\t(No symbol) [0x006D1B7F]\n\t(No symbol) [0x006BC8E8]\n\t(No symbol) [0x006BC809]\n\t(No symbol) [0x006D3DB0]\n\t(No symbol) [0x0074C3F6]\n\t(No symbol) [0x00733736]\n\t(No symbol) [0x00707541]\n\t(No symbol) [0x007080BD]\n\tGetHandleVerifier [0x00AF3A93+2876371]\n\tGetHandleVerifier [0x00B47F5D+3221661]\n\tGetHandleVerifier [0x008BD634+556916]\n\tGetHandleVerifier [0x008C474C+585868]\n\t(No symbol) [0x007DCE04]\n\t(No symbol) [0x007D9818]\n\t(No symbol) [0x007D99B7]\n\t(No symbol) [0x007CBF0E]\n\tBaseThreadInitThunk [0x76857BA9+25]\n\tRtlInitializeExceptionChain [0x7788C10B+107]\n\tRtlClearBits [0x7788C08F+191]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 210\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo query provided. Exiting.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 210\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 176\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    173\u001b[0m loader \u001b[38;5;241m=\u001b[39m GoogleSearchLoader(query\u001b[38;5;241m=\u001b[39mquery)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Load and scroll through the search results, then save the HTML content\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m html_content \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_scroll\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m html_filename \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39msave_html(html_content)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHTML content saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhtml_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 60\u001b[0m, in \u001b[0;36mGoogleSearchLoader.load_and_scroll\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m     search_items \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv.g\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     58\u001b[0m     num_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(search_items)\n\u001b[1;32m---> 60\u001b[0m html_content \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_source\u001b[49m\n\u001b[0;32m     61\u001b[0m driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m html_content\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:455\u001b[0m, in \u001b[0;36mWebDriver.page_source\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpage_source\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    448\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets the source of the current page.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    :Usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m            driver.page_source\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_PAGE_SOURCE\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\scrape_tester_100\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: disconnected: not connected to DevTools\n  (failed to check if window was closed: disconnected: not connected to DevTools)\n  (Session info: chrome=126.0.6478.127)\nStacktrace:\n\tGetHandleVerifier [0x0083C1C3+27395]\n\t(No symbol) [0x007D3DC4]\n\t(No symbol) [0x006D1B7F]\n\t(No symbol) [0x006BC8E8]\n\t(No symbol) [0x006BC809]\n\t(No symbol) [0x006D3DB0]\n\t(No symbol) [0x0074C3F6]\n\t(No symbol) [0x00733736]\n\t(No symbol) [0x00707541]\n\t(No symbol) [0x007080BD]\n\tGetHandleVerifier [0x00AF3A93+2876371]\n\tGetHandleVerifier [0x00B47F5D+3221661]\n\tGetHandleVerifier [0x008BD634+556916]\n\tGetHandleVerifier [0x008C474C+585868]\n\t(No symbol) [0x007DCE04]\n\t(No symbol) [0x007D9818]\n\t(No symbol) [0x007D99B7]\n\t(No symbol) [0x007CBF0E]\n\tBaseThreadInitThunk [0x76857BA9+25]\n\tRtlInitializeExceptionChain [0x7788C10B+107]\n\tRtlClearBits [0x7788C08F+191]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    \"\"\"Generate a filename with a timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "class GoogleSearchLoader:\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def load_and_scroll(self):\n",
    "        \"\"\"Load search results and scroll until at least 100 items or no new items are found.\"\"\"\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(f\"https://www.google.com/search?q={quote_plus(self.query)}\")\n",
    "\n",
    "        num_results = 0\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while num_results < 100:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(20)  # Increased delay to allow more content to load\n",
    "            \n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, 'a#pnnext')\n",
    "                    next_button.click()\n",
    "                    time.sleep(20)  # Wait for the next page to load\n",
    "                    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        more_results_button = driver.find_element(By.XPATH, '//a[contains(., \"More results\")]')\n",
    "                        more_results_button.click()\n",
    "                        time.sleep(20)  # Wait for more results to load\n",
    "                        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    except Exception:\n",
    "                        print(\"No more pages or unable to load more results.\")\n",
    "                        break  # No new items or next/more results button was not found\n",
    "            else:\n",
    "                last_height = new_height\n",
    "            \n",
    "            search_items = driver.find_elements(By.CSS_SELECTOR, 'div.g')\n",
    "            num_results = len(search_items)\n",
    "        \n",
    "        html_content = driver.page_source\n",
    "        driver.quit()\n",
    "        return html_content\n",
    "\n",
    "    def parse_items(self, html_content):\n",
    "        \"\"\"Parse search items from HTML content using BeautifulSoup.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        results = []\n",
    "        search_items = soup.select('div.g')  # Adjust the selector as needed\n",
    "        for item in search_items:\n",
    "            title_div = item.select_one('h3')\n",
    "            title = title_div.text if title_div else \"No title\"\n",
    "            link_tag = item.select_one('a')\n",
    "            link = link_tag['href'] if link_tag else None\n",
    "            snippet_div = item.select_one('div.kb0PBd')\n",
    "            snippet = snippet_div.text if snippet_div else \"No snippet available\"\n",
    "            if link:\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "        return results\n",
    "\n",
    "    def save_html(self, html_content, base_filename=\"google_search_results.html\"):\n",
    "        \"\"\"Save the HTML content to a timestamped file.\"\"\"\n",
    "        timestamped_file = timestamped_filename(base_filename)\n",
    "        with open(timestamped_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "        return timestamped_file\n",
    "\n",
    "    def save_results_to_json(self, results, base_filename=\"parsed_search_results.json\"):\n",
    "        \"\"\"Save the parsed results to a timestamped JSON file.\"\"\"\n",
    "        json_filename = timestamped_filename(base_filename)\n",
    "        with open(json_filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "        return json_filename\n",
    "\n",
    "def get_page_metadata(link):\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(link)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        title = driver.title\n",
    "\n",
    "        metadata = {\n",
    "            \"title\": title,\n",
    "            \"description\": \"\",\n",
    "            \"keywords\": \"\"\n",
    "        }\n",
    "\n",
    "        description_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "        if description_tag:\n",
    "            metadata[\"description\"] = description_tag.get('content', '')\n",
    "\n",
    "        keywords_tag = soup.find('meta', attrs={'name': 'keywords'})\n",
    "        if keywords_tag:\n",
    "            metadata[\"keywords\"] = keywords_tag.get('content', '')\n",
    "\n",
    "        driver.quit()\n",
    "        return metadata, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def test_link(link_info):\n",
    "    link = link_info['link']\n",
    "    metadata, error = get_page_metadata(link)\n",
    "    if error:\n",
    "        return {'link': link, 'status': 'error', 'error': error}\n",
    "    else:\n",
    "        return {'link': link, 'status': 'success', 'metadata': metadata}\n",
    "\n",
    "def test_links_concurrently(search_results):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(test_link, item): item for item in search_results}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def generate_report(test_results):\n",
    "    success_count = sum(1 for result in test_results if result['status'] == 'success')\n",
    "    error_count = len(test_results) - success_count\n",
    "\n",
    "    report_lines = [\n",
    "        \"Link Scrape Test Report\",\n",
    "        f\"Total Links Tested: {len(test_results)}\",\n",
    "        f\"Total Successes: {success_count}\",\n",
    "        f\"Total Errors: {error_count}\",\n",
    "        \"\",\n",
    "        \"Details:\"\n",
    "    ]\n",
    "\n",
    "    for result in test_results:\n",
    "        if result['status'] == 'success':\n",
    "            report_lines.append(f\"SUCCESS: {result['link']} - Metadata: {result['metadata']}\")\n",
    "        else:\n",
    "            report_lines.append(f\"ERROR: {result['link']} - Error: {result['error']}\")\n",
    "\n",
    "    return \"\\n\".join(report_lines)\n",
    "\n",
    "def main():\n",
    "    # Create a Tkinter root window and hide it\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Prompt the user for the search query using Tkinter\n",
    "    query = simpledialog.askstring(\"Input\", \"Please enter the search query topic:\")\n",
    "\n",
    "    # Check if the user provided a query\n",
    "    if query:\n",
    "        loader = GoogleSearchLoader(query=query)\n",
    "        \n",
    "        # Load and scroll through the search results, then save the HTML content\n",
    "        html_content = loader.load_and_scroll()\n",
    "        html_filename = loader.save_html(html_content)\n",
    "        print(f\"HTML content saved to {html_filename}\")\n",
    "\n",
    "        # Parse the HTML content to extract search results\n",
    "        search_results = loader.parse_items(html_content)\n",
    "        print(f\"Extracted {len(search_results)} items.\")\n",
    "\n",
    "        # Save the parsed search results to a JSON file\n",
    "        json_filename = loader.save_results_to_json(search_results)\n",
    "        print(f\"Parsed search results saved to {json_filename}\")\n",
    "\n",
    "        # Load the parsed search results from the JSON file\n",
    "        with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "            search_results = json.load(f)\n",
    "\n",
    "        # Test the links concurrently and save the results\n",
    "        test_results = test_links_concurrently(search_results)\n",
    "        output_filename = timestamped_filename('links_scrape_test.json')\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Test results saved to {output_filename}\")\n",
    "\n",
    "        # Generate the report\n",
    "        report = generate_report(test_results)\n",
    "        report_filename = timestamped_filename('links_scrape_report.txt')\n",
    "        with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        print(f\"Report saved to {report_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No query provided. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages or unable to load more results.\n",
      "HTML content saved to google_search_results_20240701_171326.html\n",
      "Extracted 9 items.\n",
      "Parsed search results saved to parsed_search_results_20240701_171326.json\n",
      "Test results saved to links_scrape_test_20240701_171342.json\n",
      "Report saved to links_scrape_report_20240701_171342.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    \"\"\"Generate a filename with a timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "class GoogleSearchLoader:\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def load_and_scroll(self, retries=3):\n",
    "        \"\"\"Load search results and scroll until at least 100 items or no new items are found.\"\"\"\n",
    "        attempt = 0\n",
    "        while attempt < retries:\n",
    "            try:\n",
    "                options = Options()\n",
    "                options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "                service = Service(ChromeDriverManager().install())\n",
    "                driver = webdriver.Chrome(service=service, options=options)\n",
    "                driver.set_page_load_timeout(60)  # Set a timeout for loading pages\n",
    "                driver.get(f\"https://www.google.com/search?q={quote_plus(self.query)}\")\n",
    "\n",
    "                num_results = 0\n",
    "                last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                \n",
    "                while num_results < 100:\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(20)  # Increased delay to allow more content to load\n",
    "                    \n",
    "                    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    if new_height == last_height:\n",
    "                        try:\n",
    "                            next_button = driver.find_element(By.CSS_SELECTOR, 'a#pnnext')\n",
    "                            next_button.click()\n",
    "                            time.sleep(20)  # Wait for the next page to load\n",
    "                            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                        except Exception:\n",
    "                            try:\n",
    "                                more_results_button = driver.find_element(By.XPATH, '//a[contains(., \"More results\")]')\n",
    "                                more_results_button.click()\n",
    "                                time.sleep(20)  # Wait for more results to load\n",
    "                                last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                            except Exception:\n",
    "                                print(\"No more pages or unable to load more results.\")\n",
    "                                break  # No new items or next/more results button was not found\n",
    "                    else:\n",
    "                        last_height = new_height\n",
    "                    \n",
    "                    search_items = driver.find_elements(By.CSS_SELECTOR, 'div.g')\n",
    "                    num_results = len(search_items)\n",
    "                \n",
    "                html_content = driver.page_source\n",
    "                driver.quit()\n",
    "                return html_content\n",
    "            except WebDriverException as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                attempt += 1\n",
    "                time.sleep(10)  # Wait before retrying\n",
    "\n",
    "        raise WebDriverException(\"Failed to load and scroll Google search results after multiple attempts.\")\n",
    "\n",
    "    def parse_items(self, html_content):\n",
    "        \"\"\"Parse search items from HTML content using BeautifulSoup.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        results = []\n",
    "        search_items = soup.select('div.g')  # Adjust the selector as needed\n",
    "        for item in search_items:\n",
    "            title_div = item.select_one('h3')\n",
    "            title = title_div.text if title_div else \"No title\"\n",
    "            link_tag = item.select_one('a')\n",
    "            link = link_tag['href'] if link_tag else None\n",
    "            snippet_div = item.select_one('div.kb0PBd')\n",
    "            snippet = snippet_div.text if snippet_div else \"No snippet available\"\n",
    "            if link:\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "        return results\n",
    "\n",
    "    def save_html(self, html_content, base_filename=\"google_search_results.html\"):\n",
    "        \"\"\"Save the HTML content to a timestamped file.\"\"\"\n",
    "        timestamped_file = timestamped_filename(base_filename)\n",
    "        with open(timestamped_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "        return timestamped_file\n",
    "\n",
    "    def save_results_to_json(self, results, base_filename=\"parsed_search_results.json\"):\n",
    "        \"\"\"Save the parsed results to a timestamped JSON file.\"\"\"\n",
    "        json_filename = timestamped_filename(base_filename)\n",
    "        with open(json_filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "        return json_filename\n",
    "\n",
    "def get_page_metadata(link):\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.set_page_load_timeout(60)  # Set a timeout for loading pages\n",
    "        driver.get(link)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        title = driver.title\n",
    "\n",
    "        metadata = {\n",
    "            \"title\": title,\n",
    "            \"description\": \"\",\n",
    "            \"keywords\": \"\"\n",
    "        }\n",
    "\n",
    "        description_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "        if description_tag:\n",
    "            metadata[\"description\"] = description_tag.get('content', '')\n",
    "\n",
    "        keywords_tag = soup.find('meta', attrs={'name': 'keywords'})\n",
    "        if keywords_tag:\n",
    "            metadata[\"keywords\"] = keywords_tag.get('content', '')\n",
    "\n",
    "        driver.quit()\n",
    "        return metadata, None\n",
    "    except WebDriverException as e:\n",
    "        return None, str(e)\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def test_link(link_info):\n",
    "    link = link_info['link']\n",
    "    metadata, error = get_page_metadata(link)\n",
    "    if error:\n",
    "        return {'link': link, 'status': 'error', 'error': error}\n",
    "    else:\n",
    "        return {'link': link, 'status': 'success', 'metadata': metadata}\n",
    "\n",
    "def test_links_concurrently(search_results):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(test_link, item): item for item in search_results}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def generate_report(test_results):\n",
    "    success_count = sum(1 for result in test_results if result['status'] == 'success')\n",
    "    error_count = len(test_results) - success_count\n",
    "\n",
    "    report_lines = [\n",
    "        \"Link Scrape Test Report\",\n",
    "        f\"Total Links Tested: {len(test_results)}\",\n",
    "        f\"Total Successes: {success_count}\",\n",
    "        f\"Total Errors: {error_count}\",\n",
    "        \"\",\n",
    "        \"Details:\"\n",
    "    ]\n",
    "\n",
    "    for result in test_results:\n",
    "        if result['status'] == 'success':\n",
    "            report_lines.append(f\"SUCCESS: {result['link']} - Metadata: {result['metadata']}\")\n",
    "        else:\n",
    "            report_lines.append(f\"ERROR: {result['link']} - Error: {result['error']}\")\n",
    "\n",
    "    return \"\\n\".join(report_lines)\n",
    "\n",
    "def main():\n",
    "    # Create a Tkinter root window and hide it\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Prompt the user for the search query using Tkinter\n",
    "    query = simpledialog.askstring(\"Input\", \"Please enter the search query topic:\")\n",
    "\n",
    "    # Check if the user provided a query\n",
    "    if query:\n",
    "        loader = GoogleSearchLoader(query=query)\n",
    "        \n",
    "        # Load and scroll through the search results, then save the HTML content\n",
    "        html_content = loader.load_and_scroll()\n",
    "        html_filename = loader.save_html(html_content)\n",
    "        print(f\"HTML content saved to {html_filename}\")\n",
    "\n",
    "        # Parse the HTML content to extract search results\n",
    "        search_results = loader.parse_items(html_content)\n",
    "        print(f\"Extracted {len(search_results)} items.\")\n",
    "\n",
    "        # Save the parsed search results to a JSON file\n",
    "        json_filename = loader.save_results_to_json(search_results)\n",
    "        print(f\"Parsed search results saved to {json_filename}\")\n",
    "\n",
    "        # Load the parsed search results from the JSON file\n",
    "        with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "            search_results = json.load(f)\n",
    "\n",
    "        # Test the links concurrently and save the results\n",
    "        test_results = test_links_concurrently(search_results)\n",
    "        output_filename = timestamped_filename('links_scrape_test.json')\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Test results saved to {output_filename}\")\n",
    "\n",
    "        # Generate the report\n",
    "        report = generate_report(test_results)\n",
    "        report_filename = timestamped_filename('links_scrape_report.txt')\n",
    "        with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        print(f\"Report saved to {report_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No query provided. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages or unable to load more results.\n",
      "HTML content saved to google_search_results_20240702_185202.html\n",
      "Extracted 10 items.\n",
      "Parsed search results saved to parsed_search_results_20240702_185202.json\n",
      "Test results saved to links_scrape_test_20240702_185217.json\n",
      "Report saved to links_scrape_report_20240702_185217.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    \"\"\"Generate a filename with a timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "class GoogleSearchLoader:\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def load_and_scroll(self, retries=3):\n",
    "        \"\"\"Load search results and scroll until at least 100 items or no new items are found.\"\"\"\n",
    "        attempt = 0\n",
    "        while attempt < retries:\n",
    "            try:\n",
    "                options = Options()\n",
    "                options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "                service = Service(ChromeDriverManager().install())\n",
    "                driver = webdriver.Chrome(service=service, options=options)\n",
    "                driver.set_page_load_timeout(60)  # Set a timeout for loading pages\n",
    "                driver.get(f\"https://www.google.com/search?q={quote_plus(self.query)}\")\n",
    "\n",
    "                num_results = 0\n",
    "                last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                \n",
    "                while num_results < 100:\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(20)  # Increased delay to allow more content to load\n",
    "                    \n",
    "                    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    if new_height == last_height:\n",
    "                        try:\n",
    "                            next_button = driver.find_element(By.CSS_SELECTOR, 'a#pnnext')\n",
    "                            next_button.click()\n",
    "                            time.sleep(20)  # Wait for the next page to load\n",
    "                            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                        except Exception:\n",
    "                            try:\n",
    "                                more_results_button = driver.find_element(By.XPATH, '//a[contains(., \"More results\")]')\n",
    "                                more_results_button.click()\n",
    "                                time.sleep(20)  # Wait for more results to load\n",
    "                                last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                            except Exception:\n",
    "                                print(\"No more pages or unable to load more results.\")\n",
    "                                break  # No new items or next/more results button was not found\n",
    "                    else:\n",
    "                        last_height = new_height\n",
    "                    \n",
    "                    search_items = driver.find_elements(By.CSS_SELECTOR, 'div.g')\n",
    "                    num_results = len(search_items)\n",
    "                \n",
    "                html_content = driver.page_source\n",
    "                driver.quit()\n",
    "                return html_content\n",
    "            except WebDriverException as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                attempt += 1\n",
    "                time.sleep(10)  # Wait before retrying\n",
    "\n",
    "        raise WebDriverException(\"Failed to load and scroll Google search results after multiple attempts.\")\n",
    "\n",
    "    def parse_items(self, html_content):\n",
    "        \"\"\"Parse search items from HTML content using BeautifulSoup.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        results = []\n",
    "        search_items = soup.select('div.g')  # Adjust the selector as needed\n",
    "        for item in search_items:\n",
    "            title_div = item.select_one('h3')\n",
    "            title = title_div.text if title_div else \"No title\"\n",
    "            link_tag = item.select_one('a')\n",
    "            link = link_tag['href'] if link_tag else None\n",
    "            snippet_div = item.select_one('div.kb0PBd')\n",
    "            snippet = snippet_div.text if snippet_div else \"No snippet available\"\n",
    "            if link:\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "        return results\n",
    "\n",
    "    def save_html(self, html_content, base_filename=\"google_search_results.html\"):\n",
    "        \"\"\"Save the HTML content to a timestamped file.\"\"\"\n",
    "        timestamped_file = timestamped_filename(base_filename)\n",
    "        with open(timestamped_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "        return timestamped_file\n",
    "\n",
    "    def save_results_to_json(self, results, base_filename=\"parsed_search_results.json\"):\n",
    "        \"\"\"Save the parsed results to a timestamped JSON file.\"\"\"\n",
    "        json_filename = timestamped_filename(base_filename)\n",
    "        with open(json_filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "        return json_filename\n",
    "\n",
    "def get_page_metadata(link):\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.set_page_load_timeout(60)  # Set a timeout for loading pages\n",
    "        driver.get(link)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        title = driver.title\n",
    "\n",
    "        metadata = {\n",
    "            \"title\": title,\n",
    "            \"description\": \"\",\n",
    "            \"keywords\": \"\"\n",
    "        }\n",
    "\n",
    "        description_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "        if description_tag:\n",
    "            metadata[\"description\"] = description_tag.get('content', '')\n",
    "\n",
    "        keywords_tag = soup.find('meta', attrs={'name': 'keywords'})\n",
    "        if keywords_tag:\n",
    "            metadata[\"keywords\"] = keywords_tag.get('content', '')\n",
    "\n",
    "        driver.quit()\n",
    "        return metadata, None\n",
    "    except WebDriverException as e:\n",
    "        return None, str(e)\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def test_link(link_info):\n",
    "    link = link_info['link']\n",
    "    metadata, error = get_page_metadata(link)\n",
    "    if error:\n",
    "        return {'link': link, 'status': 'error', 'error': error}\n",
    "    else:\n",
    "        return {'link': link, 'status': 'success', 'metadata': metadata}\n",
    "\n",
    "def test_links_concurrently(search_results):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(test_link, item): item for item in search_results}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def generate_report(test_results, query):\n",
    "    success_count = sum(1 for result in test_results if result['status'] == 'success')\n",
    "    error_count = len(test_results) - success_count\n",
    "\n",
    "    report_lines = [\n",
    "        \"Link Scrape Test Report\",\n",
    "        f\"Search Query: {query}\",\n",
    "        f\"Total Links Tested: {len(test_results)}\",\n",
    "        f\"Total Successes: {success_count}\",\n",
    "        f\"Total Errors: {error_count}\",\n",
    "        \"\",\n",
    "        \"Details:\"\n",
    "    ]\n",
    "\n",
    "    for result in test_results:\n",
    "        if result['status'] == 'success':\n",
    "            report_lines.append(f\"SUCCESS: {result['link']} - Metadata: {result['metadata']}\")\n",
    "        else:\n",
    "            report_lines.append(f\"ERROR: {result['link']} - Error: {result['error']}\")\n",
    "\n",
    "    return \"\\n\".join(report_lines)\n",
    "\n",
    "def main():\n",
    "    # Create a Tkinter root window and hide it\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Prompt the user for the search query using Tkinter\n",
    "    query = simpledialog.askstring(\"Input\", \"Please enter the search query topic:\")\n",
    "\n",
    "    # Check if the user provided a query\n",
    "    if query:\n",
    "        loader = GoogleSearchLoader(query=query)\n",
    "        \n",
    "        # Load and scroll through the search results, then save the HTML content\n",
    "        html_content = loader.load_and_scroll()\n",
    "        html_filename = loader.save_html(html_content)\n",
    "        print(f\"HTML content saved to {html_filename}\")\n",
    "\n",
    "        # Parse the HTML content to extract search results\n",
    "        search_results = loader.parse_items(html_content)\n",
    "        print(f\"Extracted {len(search_results)} items.\")\n",
    "\n",
    "        # Save the parsed search results to a JSON file\n",
    "        json_filename = loader.save_results_to_json(search_results)\n",
    "        print(f\"Parsed search results saved to {json_filename}\")\n",
    "\n",
    "        # Load the parsed search results from the JSON file\n",
    "        with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "            search_results = json.load(f)\n",
    "\n",
    "        # Test the links concurrently and save the results\n",
    "        test_results = test_links_concurrently(search_results)\n",
    "        output_filename = timestamped_filename('links_scrape_test.json')\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Test results saved to {output_filename}\")\n",
    "\n",
    "        # Generate the report\n",
    "        report = generate_report(test_results, query)\n",
    "        report_filename = timestamped_filename('links_scrape_report.txt')\n",
    "        with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        print(f\"Report saved to {report_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No query provided. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape_tester_100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
