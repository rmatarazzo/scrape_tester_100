{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content saved to google_search_results_20240708_153825.html\n",
      "Extracted 103 items.\n",
      "Parsed search results saved to parsed_search_results_20240708_153825.json\n",
      "Test results saved to links_scrape_test_20240708_154038.json\n",
      "Report saved to links_scrape_report_20240708_154038.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    \"\"\"Generate a filename with a timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "class GoogleSearchLoader:\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def load_and_scroll(self):\n",
    "        \"\"\"Load search results and scroll until at least 100 items or no new items are found.\"\"\"\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.get(f\"https://www.google.com/search?q={quote_plus(self.query)}\")\n",
    "\n",
    "        num_results = 0\n",
    "        collected_html = \"\"\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        while num_results < 100:\n",
    "            # Wait for the search results to load\n",
    "            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.g')))\n",
    "            \n",
    "            search_items = driver.find_elements(By.CSS_SELECTOR, 'div.g')\n",
    "            num_results += len(search_items)\n",
    "            collected_html += driver.page_source\n",
    "\n",
    "            if num_results >= 100:\n",
    "                break\n",
    "\n",
    "            # Try to find the \"Next\" button and click it\n",
    "            try:\n",
    "                next_button = wait.until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, 'a#pnnext'))\n",
    "                )\n",
    "                next_button.click()\n",
    "            except Exception:\n",
    "                try:\n",
    "                    more_results_button = wait.until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, '//a[contains(., \"More results\")]'))\n",
    "                    )\n",
    "                    more_results_button.click()\n",
    "                except Exception:\n",
    "                    print(\"No more pages or unable to load more results.\")\n",
    "                    break  # No new items or next/more results button was not found\n",
    "        \n",
    "        driver.quit()\n",
    "        return collected_html\n",
    "\n",
    "    def save_html(self, html_content, base_filename=\"google_search_results.html\"):\n",
    "        \"\"\"Save the HTML content to a timestamped file.\"\"\"\n",
    "        timestamped_file = timestamped_filename(base_filename)\n",
    "        with open(timestamped_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "        return timestamped_file\n",
    "\n",
    "    def save_results_to_json(self, results, base_filename=\"parsed_search_results.json\"):\n",
    "        \"\"\"Save the parsed results to a timestamped JSON file.\"\"\"\n",
    "        json_filename = timestamped_filename(base_filename)\n",
    "        with open(json_filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "        return json_filename\n",
    "\n",
    "    def parse_items(self, html_content):\n",
    "        \"\"\"Parse search items from HTML content using BeautifulSoup.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        results = []\n",
    "        search_items = soup.select('div.g')  # Adjust the selector as needed\n",
    "        for item in search_items:\n",
    "            title_div = item.select_one('h3')\n",
    "            title = title_div.text if title_div else \"No title\"\n",
    "            link_tag = item.select_one('a')\n",
    "            link = link_tag['href'] if link_tag and 'href' in link_tag.attrs else None\n",
    "            snippet_div = item.select_one('div.kb0PBd')\n",
    "            snippet = snippet_div.text if snippet_div else \"No snippet available\"\n",
    "            if link:\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "        return results\n",
    "\n",
    "def get_page_title(link):\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.get(link)\n",
    "        title = driver.title\n",
    "        driver.quit()\n",
    "        return title, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def test_link(link_info):\n",
    "    link = link_info['link']\n",
    "    title, error = get_page_title(link)\n",
    "    if error:\n",
    "        return {'link': link, 'status': 'error', 'error': error}\n",
    "    else:\n",
    "        return {'link': link, 'status': 'success', 'title': title}\n",
    "\n",
    "def test_links_concurrently(search_results):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(test_link, item): item for item in search_results}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def generate_report(test_results, query_topic):\n",
    "    success_count = sum(1 for result in test_results if result['status'] == 'success')\n",
    "    error_count = len(test_results) - success_count\n",
    "\n",
    "    report_lines = [\n",
    "        f\"Link Scrape Test Report for query: {query_topic}\",\n",
    "        f\"Total Links Tested: {len(test_results)}\",\n",
    "        f\"Total Successes: {success_count}\",\n",
    "        f\"Total Errors: {error_count}\",\n",
    "        \"\",\n",
    "        \"Details:\"\n",
    "    ]\n",
    "\n",
    "    for result in test_results:\n",
    "        if result['status'] == 'success':\n",
    "            report_lines.append(f\"SUCCESS: {result['link']} - Title: {result['title']}\")\n",
    "        else:\n",
    "            report_lines.append(f\"ERROR: {result['link']} - Error: {result['error']}\")\n",
    "\n",
    "    return \"\\n\".join(report_lines)\n",
    "\n",
    "def main():\n",
    "    # Create a Tkinter root window and hide it\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Prompt the user for the search query using Tkinter\n",
    "    query = simpledialog.askstring(\"Input\", \"Please enter the search query topic:\")\n",
    "\n",
    "    # Check if the user provided a query\n",
    "    if query:\n",
    "        loader = GoogleSearchLoader(query=query)\n",
    "        \n",
    "        # Load and scroll through the search results, then save the HTML content\n",
    "        html_content = loader.load_and_scroll()\n",
    "        html_filename = loader.save_html(html_content)\n",
    "        print(f\"HTML content saved to {html_filename}\")\n",
    "\n",
    "        # Parse the HTML content to extract search results\n",
    "        search_results = loader.parse_items(html_content)\n",
    "        print(f\"Extracted {len(search_results)} items.\")\n",
    "\n",
    "        # Save the parsed search results to a JSON file\n",
    "        json_filename = loader.save_results_to_json(search_results)\n",
    "        print(f\"Parsed search results saved to {json_filename}\")\n",
    "\n",
    "        # Load the parsed search results from the JSON file\n",
    "        with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "            search_results = json.load(f)\n",
    "\n",
    "        # Test the links concurrently and save the results\n",
    "        test_results = test_links_concurrently(search_results)\n",
    "        output_filename = timestamped_filename('links_scrape_test.json')\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Test results saved to {output_filename}\")\n",
    "\n",
    "        # Generate the report\n",
    "        report = generate_report(test_results, query)\n",
    "        report_filename = timestamped_filename('links_scrape_report.txt')\n",
    "        with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        print(f\"Report saved to {report_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No query provided. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages or unable to load more results.\n",
      "HTML content saved to google_search_results_20240708_160333.html\n",
      "Extracted 13 items.\n",
      "Parsed search results saved to parsed_search_results_20240708_160333.json\n",
      "Test results saved to links_scrape_test_20240708_160353.json\n",
      "Report saved to links_scrape_report_20240708_160353.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    \"\"\"Generate a filename with a timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "class GoogleSearchLoader:\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def load_and_scroll(self):\n",
    "        \"\"\"Load search results and scroll until at least 100 items or no new items are found.\"\"\"\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.get(f\"https://www.google.com/search?q={quote_plus(self.query)}\")\n",
    "\n",
    "        num_results = 0\n",
    "        collected_html = \"\"\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        while num_results < 100:\n",
    "            # Wait for the search results to load\n",
    "            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.g')))\n",
    "            \n",
    "            search_items = driver.find_elements(By.CSS_SELECTOR, 'div.g')\n",
    "            num_results += len(search_items)\n",
    "            collected_html += driver.page_source\n",
    "\n",
    "            if num_results >= 100:\n",
    "                break\n",
    "\n",
    "            # Try to find the \"Next\" button and click it\n",
    "            try:\n",
    "                next_button = wait.until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, 'a#pnnext'))\n",
    "                )\n",
    "                next_button.click()\n",
    "            except Exception:\n",
    "                try:\n",
    "                    more_results_button = wait.until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, '//a[contains(., \"More results\")]'))\n",
    "                    )\n",
    "                    more_results_button.click()\n",
    "                except Exception:\n",
    "                    print(\"No more pages or unable to load more results.\")\n",
    "                    break  # No new items or next/more results button was not found\n",
    "        \n",
    "        driver.quit()\n",
    "        return collected_html\n",
    "\n",
    "    def save_html(self, html_content, base_filename=\"google_search_results.html\"):\n",
    "        \"\"\"Save the HTML content to a timestamped file.\"\"\"\n",
    "        timestamped_file = timestamped_filename(base_filename)\n",
    "        with open(timestamped_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "        return timestamped_file\n",
    "\n",
    "    def save_results_to_json(self, results, base_filename=\"parsed_search_results.json\"):\n",
    "        \"\"\"Save the parsed results to a timestamped JSON file.\"\"\"\n",
    "        json_filename = timestamped_filename(base_filename)\n",
    "        with open(json_filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "        return json_filename\n",
    "\n",
    "    def parse_items(self, html_content):\n",
    "        \"\"\"Parse search items from HTML content using BeautifulSoup.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        results = []\n",
    "        search_items = soup.select('div.g')  # Adjust the selector as needed\n",
    "        for item in search_items:\n",
    "            title_div = item.select_one('h3')\n",
    "            title = title_div.text if title_div else \"No title\"\n",
    "            link_tag = item.select_one('a')\n",
    "            link = link_tag['href'] if link_tag and 'href' in link_tag.attrs else None\n",
    "            snippet_div = item.select_one('div.kb0PBd')\n",
    "            snippet = snippet_div.text if snippet_div else \"No snippet available\"\n",
    "            if link:\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "        return results\n",
    "\n",
    "def get_page_metadata(link):\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.get(link)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        title = driver.title\n",
    "        description = soup.find('meta', attrs={'name': 'description'})\n",
    "        keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
    "        \n",
    "        metadata = {\n",
    "            \"title\": title,\n",
    "            \"description\": description['content'] if description else \"No description\",\n",
    "            \"keywords\": keywords['content'] if keywords else \"No keywords\"\n",
    "        }\n",
    "        \n",
    "        driver.quit()\n",
    "        return metadata, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def test_link(link_info):\n",
    "    link = link_info['link']\n",
    "    metadata, error = get_page_metadata(link)\n",
    "    if error:\n",
    "        return {'link': link, 'status': 'error', 'error': error}\n",
    "    else:\n",
    "        return {'link': link, 'status': 'success', **metadata}\n",
    "\n",
    "def test_links_concurrently(search_results):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(test_link, item): item for item in search_results}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def generate_report(test_results, query_topic):\n",
    "    success_count = sum(1 for result in test_results if result['status'] == 'success')\n",
    "    error_count = len(test_results) - success_count\n",
    "\n",
    "    report_lines = [\n",
    "        f\"Link Scrape Test Report for query: {query_topic}\",\n",
    "        f\"Total Links Tested: {len(test_results)}\",\n",
    "        f\"Total Successes: {success_count}\",\n",
    "        f\"Total Errors: {error_count}\",\n",
    "        \"\",\n",
    "        \"Details:\"\n",
    "    ]\n",
    "\n",
    "    for result in test_results:\n",
    "        if result['status'] == 'success':\n",
    "            report_lines.append(\n",
    "                f\"SUCCESS: {result['link']} - Title: {result['title']} - Description: {result['description']} - Keywords: {result['keywords']}\"\n",
    "            )\n",
    "        else:\n",
    "            report_lines.append(f\"ERROR: {result['link']} - Error: {result['error']}\")\n",
    "\n",
    "    return \"\\n\".join(report_lines)\n",
    "\n",
    "def main():\n",
    "    # Create a Tkinter root window and hide it\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Prompt the user for the search query using Tkinter\n",
    "    query = simpledialog.askstring(\"Input\", \"Please enter the search query topic:\")\n",
    "\n",
    "    # Check if the user provided a query\n",
    "    if query:\n",
    "        loader = GoogleSearchLoader(query=query)\n",
    "        \n",
    "        # Load and scroll through the search results, then save the HTML content\n",
    "        html_content = loader.load_and_scroll()\n",
    "        html_filename = loader.save_html(html_content)\n",
    "        print(f\"HTML content saved to {html_filename}\")\n",
    "\n",
    "        # Parse the HTML content to extract search results\n",
    "        search_results = loader.parse_items(html_content)\n",
    "        print(f\"Extracted {len(search_results)} items.\")\n",
    "\n",
    "        # Save the parsed search results to a JSON file\n",
    "        json_filename = loader.save_results_to_json(search_results)\n",
    "        print(f\"Parsed search results saved to {json_filename}\")\n",
    "\n",
    "        # Load the parsed search results from the JSON file\n",
    "        with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "            search_results = json.load(f)\n",
    "\n",
    "        # Test the links concurrently and save the results\n",
    "        test_results = test_links_concurrently(search_results)\n",
    "        output_filename = timestamped_filename('links_scrape_test.json')\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Test results saved to {output_filename}\")\n",
    "\n",
    "        # Generate the report\n",
    "        report = generate_report(test_results, query)\n",
    "        report_filename = timestamped_filename('links_scrape_report.txt')\n",
    "        with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        print(f\"Report saved to {report_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No query provided. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content saved to google_search_results_20240708_171516.html\n",
      "Extracted 101 items.\n",
      "Parsed search results saved to parsed_search_results_20240708_171516.json\n",
      "Test results saved to links_scrape_test_20240708_172032.json\n",
      "Report saved to links_scrape_report_20240708_172032.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    \"\"\"Generate a filename with a timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "class GoogleSearchLoader:\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def load_and_scroll(self):\n",
    "        \"\"\"Load search results and scroll until at least 100 items or no new items are found.\"\"\"\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.get(f\"https://www.google.com/search?q={quote_plus(self.query)}\")\n",
    "\n",
    "        num_results = 0\n",
    "        collected_html = \"\"\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        \n",
    "        while num_results < 100:\n",
    "            # Wait for the search results to load\n",
    "            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.g')))\n",
    "            \n",
    "            search_items = driver.find_elements(By.CSS_SELECTOR, 'div.g')\n",
    "            num_results += len(search_items)\n",
    "            collected_html += driver.page_source\n",
    "\n",
    "            if num_results >= 100:\n",
    "                break\n",
    "\n",
    "            # Try to find the \"Next\" button and click it\n",
    "            try:\n",
    "                next_button = wait.until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, 'a#pnnext'))\n",
    "                )\n",
    "                next_button.click()\n",
    "                time.sleep(2)  # Allow time for the page to load\n",
    "            except Exception:\n",
    "                try:\n",
    "                    more_results_button = wait.until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, '//a[contains(., \"More results\")]'))\n",
    "                    )\n",
    "                    more_results_button.click()\n",
    "                    time.sleep(2)  # Allow time for more results to load\n",
    "                except Exception:\n",
    "                    print(\"No more pages or unable to load more results.\")\n",
    "                    break  # No new items or next/more results button was not found\n",
    "        \n",
    "        driver.quit()\n",
    "        return collected_html\n",
    "\n",
    "    def save_html(self, html_content, base_filename=\"google_search_results.html\"):\n",
    "        \"\"\"Save the HTML content to a timestamped file.\"\"\"\n",
    "        timestamped_file = timestamped_filename(base_filename)\n",
    "        with open(timestamped_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "        return timestamped_file\n",
    "\n",
    "    def save_results_to_json(self, results, base_filename=\"parsed_search_results.json\"):\n",
    "        \"\"\"Save the parsed results to a timestamped JSON file.\"\"\"\n",
    "        json_filename = timestamped_filename(base_filename)\n",
    "        with open(json_filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "        return json_filename\n",
    "\n",
    "    def parse_items(self, html_content):\n",
    "        \"\"\"Parse search items from HTML content using BeautifulSoup.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        results = []\n",
    "        search_items = soup.select('div.g')  # Adjust the selector as needed\n",
    "        for item in search_items:\n",
    "            title_div = item.select_one('h3')\n",
    "            title = title_div.text if title_div else \"No title\"\n",
    "            link_tag = item.select_one('a')\n",
    "            link = link_tag['href'] if link_tag and 'href' in link_tag.attrs else None\n",
    "            snippet_div = item.select_one('div.kb0PBd')\n",
    "            snippet = snippet_div.text if snippet_div else \"No snippet available\"\n",
    "            if link:\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "        return results\n",
    "\n",
    "def get_page_metadata(link):\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.get(link)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        title = driver.title\n",
    "        description = soup.find('meta', attrs={'name': 'description'})\n",
    "        keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
    "        \n",
    "        metadata = {\n",
    "            \"title\": title,\n",
    "            \"description\": description['content'] if description else \"No description\",\n",
    "            \"keywords\": keywords['content'] if keywords else \"No keywords\"\n",
    "        }\n",
    "        \n",
    "        driver.quit()\n",
    "        return metadata, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def test_link(link_info):\n",
    "    link = link_info['link']\n",
    "    metadata, error = get_page_metadata(link)\n",
    "    if error:\n",
    "        return {'link': link, 'status': 'error', 'error': error}\n",
    "    else:\n",
    "        return {'link': link, 'status': 'success', **metadata}\n",
    "\n",
    "def test_links_concurrently(search_results):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(test_link, item): item for item in search_results}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def generate_report(test_results, query_topic):\n",
    "    success_count = sum(1 for result in test_results if result['status'] == 'success')\n",
    "    error_count = len(test_results) - success_count\n",
    "\n",
    "    report_lines = [\n",
    "        f\"Link Scrape Test Report for query: {query_topic}\",\n",
    "        f\"Total Links Tested: {len(test_results)}\",\n",
    "        f\"Total Successes: {success_count}\",\n",
    "        f\"Total Errors: {error_count}\",\n",
    "        \"\",\n",
    "        \"Details:\"\n",
    "    ]\n",
    "\n",
    "    for result in test_results:\n",
    "        if result['status'] == 'success':\n",
    "            report_lines.append(\n",
    "                f\"SUCCESS: {result['link']} - Title: {result['title']} - Description: {result['description']} - Keywords: {result['keywords']}\"\n",
    "            )\n",
    "        else:\n",
    "            report_lines.append(f\"ERROR: {result['link']} - Error: {result['error']}\")\n",
    "\n",
    "    return \"\\n\".join(report_lines)\n",
    "\n",
    "def main():\n",
    "    # Create a Tkinter root window and hide it\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Prompt the user for the search query using Tkinter\n",
    "    query = simpledialog.askstring(\"Input\", \"Please enter the search query topic:\")\n",
    "\n",
    "    # Check if the user provided a query\n",
    "    if query:\n",
    "        loader = GoogleSearchLoader(query=query)\n",
    "        \n",
    "        # Load and scroll through the search results, then save the HTML content\n",
    "        html_content = loader.load_and_scroll()\n",
    "        html_filename = loader.save_html(html_content)\n",
    "        print(f\"HTML content saved to {html_filename}\")\n",
    "\n",
    "        # Parse the HTML content to extract search results\n",
    "        search_results = loader.parse_items(html_content)\n",
    "        print(f\"Extracted {len(search_results)} items.\")\n",
    "\n",
    "        # Ensure we have at least 100 unique links\n",
    "        if len(search_results) < 100:\n",
    "            print(f\"Collected only {len(search_results)} links, retrying to get more...\")\n",
    "            while len(search_results) < 100:\n",
    "                html_content = loader.load_and_scroll()\n",
    "                search_results.extend(loader.parse_items(html_content))\n",
    "                search_results = list({item['link']:item for item in search_results if item['link']}.values())\n",
    "                print(f\"Collected {len(search_results)} links so far...\")\n",
    "\n",
    "        # Save the parsed search results to a JSON file\n",
    "        json_filename = loader.save_results_to_json(search_results[:100])  # Ensure we have at most 100 links\n",
    "        print(f\"Parsed search results saved to {json_filename}\")\n",
    "\n",
    "        # Load the parsed search results from the JSON file\n",
    "        with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "            search_results = json.load(f)\n",
    "\n",
    "        # Test the links concurrently and save the results\n",
    "        test_results = test_links_concurrently(search_results)\n",
    "        output_filename = timestamped_filename('links_scrape_test.json')\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Test results saved to {output_filename}\")\n",
    "\n",
    "        # Generate the report\n",
    "        report = generate_report(test_results, query)\n",
    "        report_filename = timestamped_filename('links_scrape_report.txt')\n",
    "        with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        print(f\"Report saved to {report_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No query provided. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content saved to google_search_results_20240708_164810.html\n",
      "Extracted 102 items.\n",
      "Parsed search results saved to parsed_search_results_20240708_164810.json\n",
      "Test results saved to links_scrape_test_20240708_165003.json\n",
      "Report saved to links_scrape_report_20240708_165003.txt\n",
      "CSV report saved to links_scrape_report_20240708_165003.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "import csv\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    \"\"\"Generate a filename with a timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "class GoogleSearchLoader:\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def load_and_scroll(self):\n",
    "        \"\"\"Load search results and scroll until at least 100 items or no new items are found.\"\"\"\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        driver.get(f\"https://www.google.com/search?q={quote_plus(self.query)}\")\n",
    "\n",
    "        num_results = 0\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while num_results < 100:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)  # Allow time for content to load\n",
    "            \n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                try:\n",
    "                    next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'a#pnnext')))\n",
    "                    next_button.click()\n",
    "                    time.sleep(2)  # Wait for the next page to load\n",
    "                    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        more_results_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//a[contains(., \"More results\")]')))\n",
    "                        more_results_button.click()\n",
    "                        time.sleep(2)  # Wait for more results to load\n",
    "                        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    except Exception:\n",
    "                        print(\"No more pages or unable to load more results.\")\n",
    "                        break  # No new items or next/more results button was not found\n",
    "            else:\n",
    "                last_height = new_height\n",
    "            \n",
    "            search_items = driver.find_elements(By.CSS_SELECTOR, 'div.g')\n",
    "            num_results = len(search_items)\n",
    "        \n",
    "        html_content = driver.page_source\n",
    "        driver.quit()\n",
    "        return html_content\n",
    "\n",
    "    def save_html(self, html_content, base_filename=\"google_search_results.html\"):\n",
    "        \"\"\"Save the HTML content to a timestamped file.\"\"\"\n",
    "        timestamped_file = timestamped_filename(base_filename)\n",
    "        with open(timestamped_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "        return timestamped_file\n",
    "\n",
    "    def save_results_to_json(self, results, base_filename=\"parsed_search_results.json\"):\n",
    "        \"\"\"Save the parsed results to a timestamped JSON file.\"\"\"\n",
    "        json_filename = timestamped_filename(base_filename)\n",
    "        with open(json_filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "        return json_filename\n",
    "\n",
    "    def parse_items(self, html_content):\n",
    "        \"\"\"Parse search items from HTML content using BeautifulSoup.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        results = []\n",
    "        search_items = soup.select('div.g')  # Adjust the selector as needed\n",
    "        for item in search_items:\n",
    "            title_div = item.select_one('h3')\n",
    "            title = title_div.text if title_div else \"No title\"\n",
    "            link_tag = item.select_one('a')\n",
    "            link = link_tag['href'] if link_tag and 'href' in link_tag.attrs else None\n",
    "            snippet_div = item.select_one('div.kb0PBd')\n",
    "            snippet = snippet_div.text if snippet_div else \"No snippet available\"\n",
    "            if link:\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "        return results\n",
    "\n",
    "def get_page_metadata(link):\n",
    "    \"\"\"Fetch metadata (title, description, keywords) from a given link.\"\"\"\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(link)\n",
    "        title = driver.title\n",
    "\n",
    "        # Extract meta description and keywords\n",
    "        description = driver.execute_script(\n",
    "            \"return document.querySelector('meta[name=\\\"description\\\"]').getAttribute('content');\"\n",
    "        ) or \"No description available\"\n",
    "        keywords = driver.execute_script(\n",
    "            \"return document.querySelector('meta[name=\\\"keywords\\\"]').getAttribute('content');\"\n",
    "        ) or \"No keywords available\"\n",
    "\n",
    "        driver.quit()\n",
    "        return title, description, keywords, None\n",
    "    except Exception as e:\n",
    "        return None, None, None, str(e)\n",
    "\n",
    "def test_link(link_info):\n",
    "    link = link_info['link']\n",
    "    title, description, keywords, error = get_page_metadata(link)\n",
    "    if error:\n",
    "        return {'link': link, 'status': 'error', 'error': error}\n",
    "    else:\n",
    "        return {\n",
    "            'link': link, \n",
    "            'status': 'success', \n",
    "            'title': title,\n",
    "            'description': description,\n",
    "            'keywords': keywords\n",
    "        }\n",
    "\n",
    "def test_links_concurrently(search_results):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(test_link, item): item for item in search_results}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def generate_report(test_results, query_topic):\n",
    "    success_count = sum(1 for result in test_results if result['status'] == 'success')\n",
    "    error_count = len(test_results) - success_count\n",
    "\n",
    "    # Report header\n",
    "    report_lines = [\n",
    "        f\"Link Scrape Test Report for query: {query_topic}\",\n",
    "        f\"Total Links Tested: {len(test_results)}\",\n",
    "        f\"Total Successes: {success_count}\",\n",
    "        f\"Total Errors: {error_count}\",\n",
    "        \"\",\n",
    "        \"Details:\"\n",
    "    ]\n",
    "\n",
    "    # Table headers\n",
    "    report_lines.append(f\"{'Status':<10} | {'Link':<60} | {'Title':<40} | {'Description':<50} | {'Keywords':<50} | {'Error':<30}\")\n",
    "    report_lines.append('-' * 240)\n",
    "\n",
    "    for result in test_results:\n",
    "        if result['status'] == 'success':\n",
    "            report_lines.append(\n",
    "                f\"{result['status']:<10} | {result['link']:<60} | {result['title']:<40} | {result['description']:<50} | {result['keywords']:<50} | {'':<30}\"\n",
    "            )\n",
    "        else:\n",
    "            report_lines.append(\n",
    "                f\"{result['status']:<10} | {result['link']:<60} | {'':<40} | {'':<50} | {'':<50} | {result['error']:<30}\"\n",
    "            )\n",
    "\n",
    "    return \"\\n\".join(report_lines)\n",
    "\n",
    "def save_report_as_csv(test_results, query_topic, filename):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Query Topic\", query_topic])\n",
    "        writer.writerow([\"Total Links Tested\", len(test_results)])\n",
    "        writer.writerow([\"Total Successes\", sum(1 for result in test_results if result['status'] == 'success')])\n",
    "        writer.writerow([\"Total Errors\", len(test_results) - sum(1 for result in test_results if result['status'] == 'success')])\n",
    "        writer.writerow([])  # Empty row for spacing\n",
    "        writer.writerow([\"Status\", \"Link\", \"Title\", \"Description\", \"Keywords\", \"Error\"])\n",
    "\n",
    "        for result in test_results:\n",
    "            if result['status'] == 'success':\n",
    "                writer.writerow([result['status'], result['link'], result['title'], result['description'], result['keywords'], \"\"])\n",
    "            else:\n",
    "                writer.writerow([result['status'], result['link'], \"\", \"\", \"\", result['error']])\n",
    "\n",
    "def main():\n",
    "    # Create a Tkinter root window and hide it\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Prompt the user for the search query using Tkinter\n",
    "    query = simpledialog.askstring(\"Input\", \"Please enter the search query topic:\")\n",
    "\n",
    "    # Check if the user provided a query\n",
    "    if query:\n",
    "        loader = GoogleSearchLoader(query=query)\n",
    "        \n",
    "        # Load and scroll through the search results, then save the HTML content\n",
    "        html_content = loader.load_and_scroll()\n",
    "        html_filename = loader.save_html(html_content)\n",
    "        print(f\"HTML content saved to {html_filename}\")\n",
    "\n",
    "        # Parse the HTML content to extract search results\n",
    "        search_results = loader.parse_items(html_content)\n",
    "        print(f\"Extracted {len(search_results)} items.\")\n",
    "\n",
    "        # Ensure we have at least 100 unique links\n",
    "        if len(search_results) < 100:\n",
    "            print(f\"Collected only {len(search_results)} links, retrying to get more...\")\n",
    "            while len(search_results) < 100:\n",
    "                html_content = loader.load_and_scroll()\n",
    "                search_results.extend(loader.parse_items(html_content))\n",
    "                search_results = list({item['link']: item for item in search_results if item['link']}.values())\n",
    "                print(f\"Collected {len(search_results)} links so far...\")\n",
    "\n",
    "        # Save the parsed search results to a JSON file\n",
    "        json_filename = loader.save_results_to_json(search_results[:100])  # Ensure we have at most 100 links\n",
    "        print(f\"Parsed search results saved to {json_filename}\")\n",
    "\n",
    "        # Load the parsed search results from the JSON file\n",
    "        with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "            search_results = json.load(f)\n",
    "\n",
    "        # Test the links concurrently and save the results\n",
    "        test_results = test_links_concurrently(search_results)\n",
    "        output_filename = timestamped_filename('links_scrape_test.json')\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Test results saved to {output_filename}\")\n",
    "\n",
    "        # Generate the report and save as text\n",
    "        report = generate_report(test_results, query)\n",
    "        report_filename = timestamped_filename('links_scrape_report.txt')\n",
    "        with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        print(f\"Report saved to {report_filename}\")\n",
    "\n",
    "        # Save the report as CSV\n",
    "        csv_report_filename = timestamped_filename('links_scrape_report.csv')\n",
    "        save_report_as_csv(test_results, query, csv_report_filename)\n",
    "        print(f\"CSV report saved to {csv_report_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No query provided. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape_tester_100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
