{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Langchain Loaders to Capture and Store Search Engine Results\n",
    "\n",
    "Code and Answers below from ChatGPT 4o \n",
    "\n",
    "- Created on: 2024.06.17, 11:00:00 PM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USER\n",
    "How do I use Python with the Langchain loader to capture the results from a web search and then parse out all URLs with their search record metadata.  Provide complete examples for Google, DuckDuckGo, and Bing.  Make sure to use the free SERP API only.  I don't want to have to authenticate.\n",
    "\n",
    "## ASSISTANT\n",
    "To capture the results from a web search and parse out all URLs with their search record metadata using Python with LangChain, you can use the free SERP API. Here’s how you can do it for Google, DuckDuckGo, and Bing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Prerequisites\n",
    "1. **Install necessary libraries:**\n",
    "   You need to install `requests` for making HTTP requests and `LangChain` for handling the search results.\n",
    "\n",
    "   ```bash\n",
    "   pip install requests langchain\n",
    "   ```\n",
    "\n",
    "### Google Search Example\n",
    "\n",
    "1. **Set up the SERP API endpoint and parameters:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "***Replace `\"YOUR_API_KEY\"` with the actual free API key you get from SERP API.***\n",
    "\n",
    "This example demonstrates how to perform searches on Google, DuckDuckGo, and Bing \n",
    "using the free SERP API, parse the results, and integrate it with LangChain for a custom loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import os \n",
    "serpapi_api_key = os.environ[\"serpapi_api_key\"]\n",
    "\n",
    "def google_search(query, serpapi_api_key):\n",
    "    url = \"https://serpapi.com/search\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"location\": \"United States\",\n",
    "        \"hl\": \"en\",\n",
    "        \"gl\": \"us\",\n",
    "        \"api_key\": serpapi_api_key  # Use the free key provided by SERP API\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    #print(f\"Request URL: {response.url}\")\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    return response.json()\n",
    "\n",
    "def parse_google_results(results):\n",
    "    parsed_results = []\n",
    "    for result in results.get('organic_results', []):\n",
    "        parsed_results.append({\n",
    "            'title': result.get('title'),\n",
    "            'link': result.get('link'),\n",
    "            'snippet': result.get('snippet')\n",
    "        })\n",
    "    return parsed_results\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def save_results_to_file(query, results):\n",
    "    # Replace spaces with underscores and remove characters not allowed in Windows filenames\n",
    "    valid_filename = query.replace(\" \", \"_\").translate({ord(c): None for c in '<>:\"/\\\\|?*'})\n",
    "    \n",
    "    # Append the current datetime stamp to the filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%Mh\")\n",
    "    filename = f\"{valid_filename}_{timestamp}.json\"\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(results, file, indent=4)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    query = \"Python web scraping\"\n",
    "    results = google_search(query, serpapi_api_key)\n",
    "    parsed_results = parse_google_results(results)\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    save_results_to_file(query, results)\n",
    "    \n",
    "    print(f\"Total results: {len(parsed_results)}\")\n",
    "    for result in parsed_results:\n",
    "        print(result)\n",
    "        \n",
    "        \n",
    "# TODO: check monthly account usage so far: \n",
    "# TODO:     https://serpapi.com/account?api_key=SECRET_API_KEY check monthly account usage so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2 query for \"using Python langchain Search Results Loader youtube.com\"\n",
    "\n",
    "This version only returns 10 results per page.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "serpapi_api_key = os.environ[\"serpapi_api_key\"]\n",
    "\n",
    "def google_search(query, serpapi_api_key):\n",
    "    url = \"https://serpapi.com/search\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"location\": \"United States\",\n",
    "        \"hl\": \"en\",\n",
    "        \"gl\": \"us\",\n",
    "        \"api_key\": serpapi_api_key  # Use the free key provided by SERP API\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    #print(f\"Request URL: {response.url}\\n\")\n",
    "    print(f\"Status Code: {response.status_code}\\n\")\n",
    "    return response.json()\n",
    "\n",
    "def parse_google_results(results):\n",
    "    parsed_results = []\n",
    "    for result in results.get('organic_results', []):\n",
    "        parsed_results.append({\n",
    "            'title': result.get('title'),\n",
    "            'link': result.get('link'),\n",
    "            'snippet': result.get('snippet')\n",
    "        })\n",
    "    return parsed_results\n",
    "\n",
    "def save_results_to_file(query, results):\n",
    "    # Replace spaces with underscores and remove characters not allowed in Windows filenames\n",
    "    valid_filename = query.replace(\" \", \"_\").translate({ord(c): None for c in '<>:\"/\\\\|?*'})\n",
    "    \n",
    "    # Append the current datetime stamp to the filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%Mh\")\n",
    "    filename = f\"{valid_filename}_{timestamp}.json\"\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(results, file, indent=4)\n",
    "    print(f\"Results saved to {filename}\\n\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # query = \"Python web scraping\"\n",
    "    query = \"using Python langchain Search Results Loader youtube.com\"\n",
    "    \n",
    "    # response.json() is returned from google_search() and assigned to `results`\n",
    "    results = google_search(query, serpapi_api_key)     \n",
    "    parsed_results = parse_google_results(results)\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    save_results_to_file(query, results)\n",
    "    \n",
    "    print(f\"Total results: {len(parsed_results)}\\n\")\n",
    "    for result in parsed_results:\n",
    "        print(result)\n",
    "        \n",
    "        \n",
    "# TODO: check monthly account usage so far: \n",
    "# TODO:     https://serpapi.com/account?api_key=SECRET_API_KEY check monthly account usage so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V3 - Count Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import os \n",
    "serpapi_api_key = os.environ[\"serpapi_api_key\"]\n",
    "\n",
    "def google_search_results_count(query, serpapi_api_key):\n",
    "    url = \"https://serpapi.com/search\"\n",
    "    params = {\"q\": query, \"location\": \"United States\", \"hl\": \"en\", \"gl\": \"us\",\n",
    "        \"api_key\": serpapi_api_key  # Use the free key provided by SERP API\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    #print(f\"Request URL: {response.url}\")\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        results_count = response_json.get('search_information', {}).get('total_results')\n",
    "        print(f\"Number of search results returned: {results_count}\")\n",
    "        return response_json, results_count\n",
    "    else:\n",
    "        return {}, 0\n",
    "\n",
    "def parse_google_results(results):\n",
    "    parsed_results = []\n",
    "    for result in results.get('organic_results', []):\n",
    "        parsed_results.append({\n",
    "            'title': result.get('title'),\n",
    "            'link': result.get('link'),\n",
    "            'snippet': result.get('snippet')\n",
    "        })\n",
    "    return parsed_results\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def save_results_to_file(query, results):\n",
    "    # Replace spaces with underscores and remove characters not allowed in Windows filenames\n",
    "    valid_filename = query.replace(\" \", \"_\").translate({ord(c): None for c in '<>:\"/\\\\|?*'})\n",
    "    \n",
    "    # Append the current datetime stamp to the filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%Mh\")\n",
    "    filename = f\"{valid_filename}_{timestamp}.json\"\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(results, file, indent=4)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # query = \"Python web scraping\"\n",
    "    query = \"using Python langchain Search Results Loader youtube.com\"\n",
    "    \n",
    "    response_json, results_count = google_search_results_count(query, serpapi_api_key)\n",
    "\n",
    "    parsed_results = parse_google_results(results)\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    save_results_to_file(query, results)\n",
    "    \n",
    "    print(f\"Total results: {len(parsed_results)}\")\n",
    "    for result in parsed_results:\n",
    "        print(result)\n",
    "        \n",
    "        \n",
    "# TODO: check monthly account usage so far: \n",
    "# TODO:     https://serpapi.com/account?api_key=SECRET_API_KEY check monthly account usage so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid transmitting API keys in the URL endpoint for APIs\n",
    "\n",
    "Use alternative methods such as setting the API key in HTTP headers or using POST requests with the API key in the body, depending on what the API supports. However, it's important to check the specific API documentation because not all APIs support these methods. For SerpApi and many others, the common method is to use the API key as a query parameter, but we'll explore a general approach using headers for APIs that support it.\n",
    "\n",
    "### General Approach Using HTTP Headers\n",
    "- Check API Documentation: First, verify in the API documentation if it allows authentication through HTTP headers.\n",
    "\n",
    "- Use the Authorization Header: If supported, you can typically use the Authorization header to pass the API key or token.\n",
    "\n",
    "#### Example with Requests Library in Python\n",
    "If the API supports passing the API key through headers, you can modify your request like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import os \n",
    "serpapi_api_key = os.environ[\"serpapi_api_key\"]\n",
    "\n",
    "def google_search_results_count(query, serpapi_api_key):\n",
    "    url = \"https://serpapi.com/search\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {serpapi_api_key}\"\n",
    "    }\n",
    "    params = {\n",
    "        \"q\": query, \n",
    "        \"location\": \"United States\", \n",
    "        \"hl\": \"en\", \n",
    "        \"gl\": \"us\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        results_count = response_json.get('search_information', {}).get('total_results')\n",
    "        print(f\"Number of search results returned: {results_count}\")\n",
    "        return response_json, results_count\n",
    "    else:\n",
    "        return {}, 0\n",
    "\n",
    "def parse_google_results(results):\n",
    "    parsed_results = []\n",
    "    for result in results.get('organic_results', []):\n",
    "        parsed_results.append({\n",
    "            'title': result.get('title'),\n",
    "            'link': result.get('link'),\n",
    "            'snippet': result.get('snippet')\n",
    "        })\n",
    "    return parsed_results\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def save_results_to_file(query, results):\n",
    "    # Replace spaces with underscores and remove characters not allowed in Windows filenames\n",
    "    valid_filename = query.replace(\" \", \"_\").translate({ord(c): None for c in '<>:\"/\\\\|?*'})\n",
    "    \n",
    "    # Append the current datetime stamp to the filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%Mh\")\n",
    "    filename = f\"{valid_filename}_{timestamp}.json\"\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(results, file, indent=4)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # query = \"Python web scraping\"\n",
    "    query = \"using Python langchain Search Results Loader youtube.com\"\n",
    "    \n",
    "    response_json, results_count = google_search_results_count(query, serpapi_api_key)\n",
    "\n",
    "    parsed_results = parse_google_results(results)\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    save_results_to_file(query, results)\n",
    "    \n",
    "    print(f\"Total results: {len(parsed_results)}\")\n",
    "    for result in parsed_results:\n",
    "        print(result)\n",
    "        \n",
    "        \n",
    "# TODO: check monthly account usage so far: \n",
    "# TODO:     https://serpapi.com/account?api_key=SECRET_API_KEY check monthly account usage so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: DuckDuckGo Search Example\n",
    "\n",
    "##### 1. **Set up the SERP API endpoint and parameters:**\n",
    "\n",
    "import requests\n",
    "\n",
    "def duckduckgo_search(query):\n",
    "    url = \"https://serpapi.com/search\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"location\": \"United States\",\n",
    "        \"hl\": \"en\",\n",
    "        \"gl\": \"us\",\n",
    "        \"api_key\": \"YOUR_API_KEY\",\n",
    "        \"engine\": \"duckduckgo\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "def parse_duckduckgo_results(results):\n",
    "    parsed_results = []\n",
    "    for result in results.get('organic_results', []):\n",
    "        parsed_results.append({\n",
    "            'title': result.get('title'),\n",
    "            'link': result.get('link'),\n",
    "            'snippet': result.get('snippet')\n",
    "        })\n",
    "    return parsed_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"Python web scraping\"\n",
    "    results = duckduckgo_search(query)\n",
    "    parsed_results = parse_duckduckgo_results(results)\n",
    "    for result in parsed_results:\n",
    "        print(result)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### TODO: Bing Search Example\n",
    "\n",
    "# 1. **Set up the SERP API endpoint and parameters:**\n",
    "\n",
    "import requests\n",
    "\n",
    "def bing_search(query):\n",
    "    url = \"https://serpapi.com/search\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"location\": \"United States\",\n",
    "        \"hl\": \"en\",\n",
    "        \"gl\": \"us\",\n",
    "        \"api_key\": \"YOUR_API_KEY\",\n",
    "        \"engine\": \"bing\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "def parse_bing_results(results):\n",
    "    parsed_results = []\n",
    "    for result in results.get('organic_results', []):\n",
    "        parsed_results.append({\n",
    "            'title': result.get('title'),\n",
    "            'link': result.get('link'),\n",
    "            'snippet': result.get('snippet')\n",
    "        })\n",
    "    return parsed_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"Python web scraping\"\n",
    "    results = bing_search(query)\n",
    "    parsed_results = parse_bing_results(results)\n",
    "    for result in parsed_results:\n",
    "        print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complete Example with LangChain Integration\n",
    "\n",
    "### To integrate this with LangChain, you need to create a custom `Loader` class that uses these search functions.\n",
    "\n",
    "#### 1. **Create the custom loader:**\n",
    "\n",
    "from langchain.loaders import BaseLoader\n",
    "\n",
    "class SERPLoader(BaseLoader):\n",
    "    def __init__(self, engine, query):\n",
    "        self.engine = engine\n",
    "        self.query = query\n",
    "    def load(self):\n",
    "        if self.engine == \"google\":\n",
    "            return google_search(self.query)\n",
    "        elif self.engine == \"duckduckgo\":\n",
    "            return duckduckgo_search(self.query)\n",
    "        elif self.engine == \"bing\":\n",
    "            return bing_search(self.query)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported search engine\")\n",
    "    def parse(self, results):\n",
    "        if self.engine == \"google\":\n",
    "            return parse_google_results(results)\n",
    "        elif self.engine == \"duckduckgo\":\n",
    "            return parse_duckduckgo_results(results)\n",
    "        elif self.engine == \"bing\":\n",
    "            return parse_bing_results(results)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported search engine\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = SERPLoader(engine=\"google\", query=\"Python web scraping\")\n",
    "    results = loader.load()\n",
    "    parsed_results = loader.parse(results)\n",
    "    for result in parsed_results:\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use LangChain loader to get search engine results without SERP API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USER\n",
    "Is there another langchain loader to scrape results from search engines that does not use SERPAPI.com ?\n",
    "\n",
    "## ASSISTANT\n",
    "Yes, you can use the LangChain loader to scrape results directly from search engines without using SERP APIa. One common approach is to use BeautifulSoup along with requests to scrape the search engine results page (SERP). Here's how you can create custom loaders for Google, DuckDuckGo, and Bing.\n",
    "\n",
    "### Prerequisites\n",
    "1. **Install necessary libraries:**\n",
    "   You need to install `requests`, `beautifulsoup4`, and `LangChain`.\n",
    "\n",
    "   ```bash\n",
    "   pip install requests beautifulsoup4 langchain\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google Search Example\n",
    "\n",
    "### 1. **Set up the Google search loader:**\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#from langchain.loaders import BaseLoader\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "\n",
    "\n",
    "\n",
    "class GoogleSearchLoader(BaseLoader):\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "    def load(self):\n",
    "        url = f\"https://www.google.com/search?q={self.query}\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        return response.text\n",
    "    def parse(self, html):\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        results = []\n",
    "        for g in soup.find_all('div', class_='BNeawe vvjwJb AP7Wnd'):\n",
    "            title = g.get_text()\n",
    "            link = g.find_parent('a')['href']\n",
    "            snippet = g.find_next_sibling('div').get_text()\n",
    "            results.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'snippet': snippet\n",
    "            })\n",
    "        return results\n",
    "if __name__ == \"__main__\":\n",
    "    loader = GoogleSearchLoader(query=\"Python web scraping\")\n",
    "    html = loader.load()\n",
    "    parsed_results = loader.parse(html)\n",
    "    for result in parsed_results:\n",
    "        print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### DuckDuckGo Search Example\n",
    "\n",
    "1. **Set up the DuckDuckGo search loader:**\n",
    "\n",
    "   ```python\n",
    "   import requests\n",
    "   from bs4 import BeautifulSoup\n",
    "   from langchain.loaders import BaseLoader\n",
    "\n",
    "   class DuckDuckGoSearchLoader(BaseLoader):\n",
    "       def __init__(self, query):\n",
    "           self.query = query\n",
    "\n",
    "       def load(self):\n",
    "           url = f\"https://duckduckgo.com/html/?q={self.query}\"\n",
    "           headers = {\n",
    "               \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "           response = requests.get(url, headers=headers)\n",
    "           return response.text\n",
    "\n",
    "       def parse(self, html):\n",
    "           soup = BeautifulSoup(html, \"html.parser\")\n",
    "           results = []\n",
    "           for result in soup.find_all('a', class_='result__a'):\n",
    "               title = result.get_text()\n",
    "               link = result['href']\n",
    "               snippet = result.find_next('a', class_='result__snippet').get_text() if result.find_next('a', class_='result__snippet') else ''\n",
    "               results.append({\n",
    "                   'title': title,\n",
    "                   'link': link,\n",
    "                   'snippet': snippet\n",
    "               })\n",
    "           return results\n",
    "\n",
    "   if __name__ == \"__main__\":\n",
    "       loader = DuckDuckGoSearchLoader(query=\"Python web scraping\")\n",
    "       html = loader.load()\n",
    "       parsed_results = loader.parse(html)\n",
    "       for result in parsed_results:\n",
    "           print(result)\n",
    "   ```\n",
    "\n",
    "### Bing Search Example\n",
    "\n",
    "1. **Set up the Bing search loader:**\n",
    "\n",
    "   ```python\n",
    "   import requests\n",
    "   from bs4 import BeautifulSoup\n",
    "   from langchain.loaders import BaseLoader\n",
    "\n",
    "   class BingSearchLoader(BaseLoader):\n",
    "       def __init__(self, query):\n",
    "           self.query = query\n",
    "\n",
    "       def load(self):\n",
    "           url = f\"https://www.bing.com/search?q={self.query}\"\n",
    "           headers = {\n",
    "               \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "           response = requests.get(url, headers=headers)\n",
    "           return response.text\n",
    "\n",
    "       def parse(self, html):\n",
    "           soup = BeautifulSoup(html, \"html.parser\")\n",
    "           results = []\n",
    "           for result in soup.find_all('li', class_='b_algo'):\n",
    "               title = result.find('h2').get_text()\n",
    "               link = result.find('a')['href']\n",
    "               snippet = result.find('p').get_text() if result.find('p') else ''\n",
    "               results.append({\n",
    "                   'title': title,\n",
    "                   'link': link,\n",
    "                   'snippet': snippet\n",
    "               })\n",
    "           return results\n",
    "\n",
    "   if __name__ == \"__main__\":\n",
    "       loader = BingSearchLoader(query=\"Python web scraping\")\n",
    "       html = loader.load()\n",
    "       parsed_results = loader.parse(html)\n",
    "       for result in parsed_results:\n",
    "           print(result)\n",
    "   ```\n",
    "\n",
    "### Complete Example with LangChain Integration\n",
    "\n",
    "To integrate this with LangChain, you can use the custom loaders created above. Here’s how to use one of them with LangChain:\n",
    "\n",
    "```python\n",
    "from langchain.loaders import BaseLoader\n",
    "\n",
    "class CustomLoader(BaseLoader):\n",
    "    def __init__(self, engine, query):\n",
    "        self.engine = engine\n",
    "        self.query = query\n",
    "\n",
    "    def load(self):\n",
    "        if self.engine == \"google\":\n",
    "            return GoogleSearchLoader(self.query).load()\n",
    "        elif self.engine == \"duckduckgo\":\n",
    "            return DuckDuckGoSearchLoader(self.query).load()\n",
    "        elif self.engine == \"bing\":\n",
    "            return BingSearchLoader(self.query).load()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported search engine\")\n",
    "\n",
    "    def parse(self, html):\n",
    "        if self.engine == \"google\":\n",
    "            return GoogleSearchLoader(self.query).parse(html)\n",
    "        elif self.engine == \"duckduckgo\":\n",
    "            return DuckDuckGoSearchLoader(self.query).parse(html)\n",
    "        elif self.engine == \"bing\":\n",
    "            return BingSearchLoader(self.query).parse(html)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported search engine\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = CustomLoader(engine=\"google\", query=\"Python web scraping\")\n",
    "    html = loader.load()\n",
    "    parsed_results = loader.parse(html)\n",
    "    for result in parsed_results:\n",
    "        print(result)\n",
    "```\n",
    "\n",
    "This example demonstrates how to perform searches on Google, DuckDuckGo, and Bing using web scraping techniques, parse the results, and integrate them with LangChain for a custom loader. Make sure to handle the search engine's terms of service and usage policies regarding web scraping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrapers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
