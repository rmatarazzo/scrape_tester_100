{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "def timestamped_filename(base_filename):\n",
    "    \"\"\"Generate a filename with a timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename, ext = os.path.splitext(base_filename)\n",
    "    return f\"{filename}_{timestamp}{ext}\"\n",
    "\n",
    "class GoogleSearchLoader:\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def load_and_scroll(self):\n",
    "        \"\"\"Load search results and scroll until at least 100 items or no new items are found.\"\"\"\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(f\"https://www.google.com/search?q={quote_plus(self.query)}\")\n",
    "\n",
    "        num_results = 0\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while num_results < 100:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(20)  # Increased delay to allow more content to load\n",
    "            \n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, 'a#pnnext')\n",
    "                    next_button.click()\n",
    "                    time.sleep(20)  # Wait for the next page to load\n",
    "                    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        more_results_button = driver.find_element(By.XPATH, '//a[contains(., \"More results\")]')\n",
    "                        more_results_button.click()\n",
    "                        time.sleep(20)  # Wait for more results to load\n",
    "                        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    except Exception:\n",
    "                        print(\"No more pages or unable to load more results.\")\n",
    "                        break  # No new items or next/more results button was not found\n",
    "            else:\n",
    "                last_height = new_height\n",
    "            \n",
    "            search_items = driver.find_elements(By.CSS_SELECTOR, 'div.g')\n",
    "            num_results = len(search_items)\n",
    "        \n",
    "        html_content = driver.page_source\n",
    "        driver.quit()\n",
    "        return html_content\n",
    "\n",
    "    def parse_items(self, html_content):\n",
    "        \"\"\"Parse search items from HTML content using BeautifulSoup.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        results = []\n",
    "        search_items = soup.select('div.g')  # Adjust the selector as needed\n",
    "        for item in search_items:\n",
    "            title_div = item.select_one('h3')\n",
    "            title = title_div.text if title_div else \"No title\"\n",
    "            link_tag = item.select_one('a')\n",
    "            link = link_tag['href'] if link_tag else None\n",
    "            snippet_div = item.select_one('div.kb0PBd')\n",
    "            snippet = snippet_div.text if snippet_div else \"No snippet available\"\n",
    "            if link:\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "        return results\n",
    "\n",
    "    def save_html(self, html_content, base_filename=\"google_search_results.html\"):\n",
    "        \"\"\"Save the HTML content to a timestamped file.\"\"\"\n",
    "        timestamped_file = timestamped_filename(base_filename)\n",
    "        with open(timestamped_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "        return timestamped_file\n",
    "\n",
    "    def save_results_to_json(self, results, base_filename=\"parsed_search_results.json\"):\n",
    "        \"\"\"Save the parsed results to a timestamped JSON file.\"\"\"\n",
    "        json_filename = timestamped_filename(base_filename)\n",
    "        with open(json_filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "        return json_filename\n",
    "\n",
    "def get_page_title(link):\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(link)\n",
    "        title = driver.title\n",
    "        driver.quit()\n",
    "        return title, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def test_link(link_info):\n",
    "    link = link_info['link']\n",
    "    title, error = get_page_title(link)\n",
    "    if error:\n",
    "        return {'link': link, 'status': 'error', 'error': error}\n",
    "    else:\n",
    "        return {'link': link, 'status': 'success', 'title': title}\n",
    "\n",
    "def test_links_concurrently(search_results):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(test_link, item): item for item in search_results}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def generate_report(test_results):\n",
    "    success_count = sum(1 for result in test_results if result['status'] == 'success')\n",
    "    error_count = len(test_results) - success_count\n",
    "\n",
    "    report_lines = [\n",
    "        \"Link Scrape Test Report\",\n",
    "        f\"Total Links Tested: {len(test_results)}\",\n",
    "        f\"Total Successes: {success_count}\",\n",
    "        f\"Total Errors: {error_count}\",\n",
    "        \"\",\n",
    "        \"Details:\"\n",
    "    ]\n",
    "\n",
    "    for result in test_results:\n",
    "        if result['status'] == 'success':\n",
    "            report_lines.append(f\"SUCCESS: {result['link']} - Title: {result['title']}\")\n",
    "        else:\n",
    "            report_lines.append(f\"ERROR: {result['link']} - Error: {result['error']}\")\n",
    "\n",
    "    return \"\\n\".join(report_lines)\n",
    "\n",
    "def main():\n",
    "    # Create a Tkinter root window and hide it\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Prompt the user for the search query using Tkinter\n",
    "    query = simpledialog.askstring(\"Input\", \"Please enter the search query topic:\")\n",
    "\n",
    "    # Check if the user provided a query\n",
    "    if query:\n",
    "        loader = GoogleSearchLoader(query=query)\n",
    "        \n",
    "        # Load and scroll through the search results, then save the HTML content\n",
    "        html_content = loader.load_and_scroll()\n",
    "        html_filename = loader.save_html(html_content)\n",
    "        print(f\"HTML content saved to {html_filename}\")\n",
    "\n",
    "        # Parse the HTML content to extract search results\n",
    "        search_results = loader.parse_items(html_content)\n",
    "        print(f\"Extracted {len(search_results)} items.\")\n",
    "\n",
    "        # Save the parsed search results to a JSON file\n",
    "        json_filename = loader.save_results_to_json(search_results)\n",
    "        print(f\"Parsed search results saved to {json_filename}\")\n",
    "\n",
    "        # Load the parsed search results from the JSON file\n",
    "        with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "            search_results = json.load(f)\n",
    "\n",
    "        # Test the links concurrently and save the results\n",
    "        test_results = test_links_concurrently(search_results)\n",
    "        output_filename = timestamped_filename('links_scrape_test.json')\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Test results saved to {output_filename}\")\n",
    "\n",
    "        # Generate the report\n",
    "        report = generate_report(test_results)\n",
    "        report_filename = timestamped_filename('links_scrape_report.txt')\n",
    "        with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        print(f\"Report saved to {report_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No query provided. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape_tester_100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
